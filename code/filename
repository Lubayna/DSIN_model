 1/1: import numpy
 1/2: !conda install numpy
 1/3: y
 1/4: import numpy
 1/5: import coremltools
 1/6:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

// model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 1/7:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 1/8: import keras
 1/9: import keras
1/10:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
1/11: import coremltools
1/12:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
1/13:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 2/1: import numpy
 2/2: import coremltools
 2/3: import keras
 2/4:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 2/5:
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
 2/6:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 2/7: import numpy
 2/8: import coremltools
 2/9: import keras
2/10:
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
2/11:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 3/1:
import tensorflow as tf
print(tf.__version__)
 4/1: import numpy
 4/2: import coremltools
 4/3: import keras
 4/4:
import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
 4/5:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.keras.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 4/6:

class_labels = ['airplane', 'automobile' ,'bird ','cat ','deer ','dog ','frog ','horse ','ship ','truck']

# model is just a trained keras model.
coreml_model = coremltools.converters.convert(model,
                                                    input_names=['image'],
                                                    image_input_names='image',
                                                    class_labels=class_labels)
coreml_model.save('CIFAR.mlmodel')
 4/7:
from sklearn.linear_model import LinearRegression
import pandas as pd

# Load data
data = pd.read_csv('houses.csv')

# Train a model
model = LinearRegression()
model.fit(data[["bedroom", "bath", "size"]], data["price"])

 # Convert and save the scikit-learn model
import coremltools
coreml_model = coremltools.converters.sklearn.convert(model,
                                                         ["bedroom", "bath", "size"],
                                                         "price")
coreml_model.save('HousePricer.mlmodel')
Next  Previous
 4/8:
from sklearn.linear_model import LinearRegression
import pandas as pd

# Load data
data = pd.read_csv('houses.csv')

# Train a model
model = LinearRegression()
model.fit(data[["bedroom", "bath", "size"]], data["price"])

 # Convert and save the scikit-learn model
import coremltools
coreml_model = coremltools.converters.sklearn.convert(model,
                                                         ["bedroom", "bath", "size"],
                                                         "price")
# coreml_model.save('HousePricer.mlmodel')
# Next  Previous
 4/9: inputs
4/10: from keras.datasets import cifar10
4/11: from keras.datasets import cifar10
4/12: (trainX, trainY), (testX, testY) = cifar10.load_data()
4/13:
# summarize the loaded data
print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))
print('Test: X=%s, y=%s' % (testX.shape, testy.shape))
4/14:
# summarize the loaded data
print('Train: X=%s, y=%s' % (trainX.shape, trainY.shape))
print('Test: X=%s, y=%s' % (testX.shape, testY.shape))
4/15:
# plot first few images
for i in range(9):
    # define subplot
    pyplot.subplot(330 + 1 + i)
    # plot raw pixel data
    pyplot.imshow(trainX[i])
# show the figure
pyplot.show()
4/16:
# plot first few images
import matplotlib
for i in range(9):
    # define subplot
    pyplot.subplot(330 + 1 + i)
    # plot raw pixel data
    pyplot.imshow(trainX[i])
# show the figure
pyplot.show()
4/17:
# plot first few images
import matplotlib.pyplot
for i in range(9):
    # define subplot
    pyplot.subplot(330 + 1 + i)
    # plot raw pixel data
    pyplot.imshow(trainX[i])
# show the figure
pyplot.show()
4/18:
# plot first few images
import matplotlib.pyplot as plt
for i in range(9):
    # define subplot
    plt.subplot(330 + 1 + i)
    # plot raw pixel data
    plt.imshow(trainX[i])
# show the figure
plt.show()
4/19:
# one hot encode target values
trainY = to_categorical(trainY)
testY = to_categorical(testY)
4/20:
# one hot encode target values
from keras.utils import to_categorical
trainY = to_categorical(trainY)
testY = to_categorical(testY)
4/21:
# one hot encode target values
from keras.utils.np_utils import to_categorical
trainY = to_categorical(trainY)
testY = to_categorical(testY)
4/22: trainY
4/23:
# テスト用のデータ生成
data = np.random.randint(low=0, high=5, size=10)
print(data)
# One-Hotベクトルに変換
print(to_categorical(data))
4/24: import numpy as np
4/25:
# テスト用のデータ生成
data = np.random.randint(low=0, high=5, size=10)
print(data)
# One-Hotベクトルに変換
print(to_categorical(data))
4/26:
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
4/27:
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
#   trainY = to_categorical(trainY)
#   testY = to_categorical(testY)
#   return trainX, trainY, testX, testY
4/28: testY
4/29: testX
4/30: testY
4/31:
# convert from integers to floats
train_norm = train.astype('float32')
test_norm = test.astype('float32')
# normalize to range 0-1
train_norm = train_norm / 255.0
test_norm = test_norm / 255.0
4/32:
# convert from integers to floats
train_norm = trainX.astype('float32')
test_norm = testX.astype('float32')
# normalize to range 0-1
train_norm = train_norm / 255.0
test_norm = test_norm / 255.0
4/33:
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
4/34:
# define cnn model
def define_model():
    model = Sequential()
    # ...
    return model
4/35:
# fit model
history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
4/36:
# define cnn model
def define_model():
    model = Sequential()
    # ...
    model.compile()
    return model
4/38:
# fit model
history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
4/39:
# define cnn model
def define_model():
    model = Sequential().compile()
    return model
4/40:
# fit model
history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
4/41:
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
4/42: run_test_harness()
4/43:
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
4/44:
# convert from integers to floats
train_norm = trainX.astype('float32')
test_norm = testX.astype('float32')
# normalize to range 0-1
train_norm = train_norm / 255.0
test_norm = test_norm / 255.0
4/45:
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
4/46:
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
4/47: run_test_harness()
4/48: from keras.models import Sequential
4/49: run_test_harness()
4/50:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    # ...
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/52:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    # ...
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/53:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    # ...
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/54:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from tensorflow.keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    # ...
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/55:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from tensorflow.keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/56:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from tensorflow.keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/57:
!pip install keras==2.2.5
!pip install tensorflow==1.14.0
!pip install h5py==2.10.0
4/58:
(trainX, trainy), (testX, testy) = cifar10.load_data()
# summarize loaded dataset
print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))
4/59: trainX.shape
4/60: trainX
4/61: 32*3
4/62: 32*32
4/63: trainy
4/64:
train_norm = trainX.astype('float32')
# normalize to range 0-1
train_norm = train_norm / 255.0
4/65: train_norm
4/66:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from tensorflow.keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/67:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/68:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils.np_utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/69:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/70:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
# from tensorflow.keras.optimizers import SGD
from keras.optimizers import gradient_descent_v2
#sgd = gradient_descent_v2.SGD(...)
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = gradient_descent_v2.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model

def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/71:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
# from tensorflow.keras.optimizers import SGD
from keras.optimizers import gradient_descent_v2
#sgd = gradient_descent_v2.SGD(...)
  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/72:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
# from tensorflow.keras.optimizers import SGD
from keras.optimizers import gradient_descent_v2
#sgd = gradient_descent_v2.SGD(...)
  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history) 
# entry point, run the test harness
run_test_harness()
4/73:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
# from tensorflow.keras.optimizers import SGD
from keras.optimizers import gradient_descent_v2
#sgd = gradient_descent_v2.SGD(...)
  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history) 
# entry point, run the test harness
run_test_harness()
4/74:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from tensorflow.keras.optimizers import SGD
#from keras.optimizers import gradient_descent_v2
#sgd = gradient_descent_v2.SGD(...)
  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history) 
# entry point, run the test harness
run_test_harness()
4/75:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from tensorflow.keras.optimizers import SGD
#from keras.optimizers import gradient_descent_v2
#sgd = gradient_descent_v2.SGD(...)
  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD()
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history) 
# entry point, run the test harness
run_test_harness()
4/76:
# test harness for evaluating models on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
# from tensorflow.keras.optimizers import SGD
from keras.optimizers import gradient_descent_v2
#sgd = gradient_descent_v2.SGD(...)
  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = gradient_descent_v2.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history) 
# entry point, run the test harness
run_test_harness()
4/77:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/78:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.optimizers import SGD
 
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/79:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import SGD
from tensorflow.keras.optimizers import SGD  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/80:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.optimizers import gradient_descent_v2
#from keras.optimizers import SGD
#from tensorflow.keras.optimizers import SGD  
# load train and test dataset
def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = gradient_descent_v2.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/81:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import gradient_descent_v2
from keras import optimizers
#from tensorflow.keras.optimizers import SGD  
# load train and test dataset

def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/82:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import gradient_descent_v2
from keras import optimizers
from tensorflow.keras import optimizers 
# load train and test dataset

def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/83:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import gradient_descent_v2
from tensorflow.keras import optimizers 
from keras import optimizers
# load train and test dataset

def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/84:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import gradient_descent_v2
from keras import optimizers
from tensorflow.keras import optimizers 
# load train and test dataset

def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/85:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
# from keras.models import Sequential
from tensorflow.keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import gradient_descent_v2
from keras import optimizers
from tensorflow.keras import optimizers 
# load train and test dataset

def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/86:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
# from keras.models import Sequential
from tensorflow.keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import gradient_descent_v2
from keras import optimizers
from tensorflow.keras import optimizers 
# load train and test dataset

def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
#   model.add(Dropout(0.2))
#   model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
#   model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
#   model.add(MaxPooling2D((2, 2)))
#   model.add(Dropout(0.2))
#   model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
#   model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
#   model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
 
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
 
# entry point, run the test harness
run_test_harness()
4/87:
# baseline model with dropout on the cifar10 dataset
import sys
from matplotlib import pyplot
from keras.datasets import cifar10
from keras.utils.np_utils import to_categorical
# from keras.models import Sequential
from tensorflow.keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
#from keras.optimizers import gradient_descent_v2
#from keras import optimizers
from tensorflow.keras import optimizers 
# load train and test dataset

def load_dataset():
    # load dataset
    (trainX, trainY), (testX, testY) = cifar10.load_data()
    # one hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY
 
# scale pixels
def prep_pixels(train, test):
    # convert from integers to floats
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    # return normalized images
    return train_norm, test_norm
 
# define cnn model
def define_model():
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))
    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Dropout(0.2))
    model.add(Flatten())
    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # compile model
    opt = optimizers.SGD(lr=0.001, momentum=0.9)
    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
 
# plot diagnostic learning curves
def summarize_diagnostics(history):
    # plot loss
    pyplot.subplot(211)
    pyplot.title('Cross Entropy Loss')
    pyplot.plot(history.history['loss'], color='blue', label='train')
    pyplot.plot(history.history['val_loss'], color='orange', label='test')
    # plot accuracy
    pyplot.subplot(212)
    pyplot.title('Classification Accuracy')
    pyplot.plot(history.history['accuracy'], color='blue', label='train')
    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')
    # save plot to file
    filename = sys.argv[0].split('/')[-1]
    pyplot.savefig(filename + '_plot.png')
    pyplot.close()
    
# run the test harness for evaluating a model
def run_test_harness():
    # load dataset
    trainX, trainY, testX, testY = load_dataset()
    # prepare pixel data
    trainX, testX = prep_pixels(trainX, testX)
    # define model
    model = define_model()
    # fit model
    history = model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), verbose=0)
    # evaluate model
    _, acc = model.evaluate(testX, testY, verbose=0)
    print('> %.3f' % (acc * 100.0))
    # learning curves
    summarize_diagnostics(history)
    
# entry point, run the test harness

run_test_harness()
4/88:
import tensorflow as tf # TF 2.2.0

# Download MobileNetv2 (using tf.keras)
keras_model = tf.keras.applications.MobileNetV2(
    weights="imagenet", 
    input_shape=(224, 224, 3,),
    classes=1000,
)
4/89:
# Download class labels (from a separate file)
import urllib
label_url = 'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt'
class_labels = urllib.request.urlopen(label_url).read().splitlines()
class_labels = class_labels[1:] # remove the first class which is background
assert len(class_labels) == 1000

# make sure entries of class_labels are strings
for i, label in enumerate(class_labels):
    if isinstance(label, bytes):
        class_labels[i] = label.decode("utf8")
4/90:
import coremltools as ct

# Define the input type as image, 
# set pre-processing parameters to normalize the image 
# to have its values in the interval [-1,1] 
# as expected by the mobilenet model
image_input = ct.ImageType(shape=(1, 224, 224, 3,),
                           bias=[-1,-1,-1], scale=1/127)

# set class labels
classifier_config = ct.ClassifierConfig(class_labels)

# Convert the model using the Unified Conversion API to an ML Program
model = ct.convert(
    keras_model, 
    convert_to="mlprogram",
    inputs=[image_input], 
    classifier_config=classifier_config,
)
4/91:
import coremltools as ct

# Define the input type as image, 
# set pre-processing parameters to normalize the image 
# to have its values in the interval [-1,1] 
# as expected by the mobilenet model
image_input = ct.ImageType(shape=(1, 224, 224, 3,),
                           bias=[-1,-1,-1], scale=1/127)

# set class labels
classifier_config = ct.ClassifierConfig(class_labels)

# Convert the model using the Unified Conversion API to a neural network
model = ct.convert(
    keras_model, 
    inputs=[image_input], 
    classifier_config=classifier_config,
)
4/92:
# Set feature descriptions (these show up as comments in XCode)
model.input_description["input_1"] = "Input image to be classified"
model.output_description["classLabel"] = "Most likely image category"

# Set model author name
model.author = '"Original Paper: Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen'

# Set the license of the model
model.license = "Please see https://github.com/tensorflow/tensorflow for license information, and https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet for the original source of the model."

# Set a short description for the Xcode UI
model.short_description = "Detects the dominant objects present in an image from a set of 1001 categories such as trees, animals, food, vehicles, person etc. The top-1 accuracy from the original publication is 74.7%."

# Set a version for the model
model.version = "2.0"
4/93: model.input_description
4/94:
# Set feature descriptions (these show up as comments in XCode)
model.input_description["input_2"] = "Input image to be classified"
model.output_description["classLabel"] = "Most likely image category"

# Set model author name
model.author = '"Original Paper: Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen'

# Set the license of the model
model.license = "Please see https://github.com/tensorflow/tensorflow for license information, and https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet for the original source of the model."

# Set a short description for the Xcode UI
model.short_description = "Detects the dominant objects present in an image from a set of 1001 categories such as trees, animals, food, vehicles, person etc. The top-1 accuracy from the original publication is 74.7%."

# Set a version for the model
model.version = "2.0"
4/95:
# Use PIL to load and resize the image to expected size
from PIL import Image
example_image = Image.open("daisy.jpg").resize((224, 224))

# Make a prediction using Core ML
out_dict = model.predict({"input_1": example_image})

# Print out top-1 prediction
print(out_dict["classLabel"])
4/96: Image
4/97:
# Save model as a Core ML model package
model.save("MobileNetV2.mlpackage")
# Load the saved model
loaded_model = ct.models.MLModel("MobileNetV2.mlpackage")
4/98:
# Save model in a Core ML mlmodel file
model.save("MobileNetV2.mlmodel")
                  
# Load the saved model
loaded_model = ct.models.MLModel("MobileNetV2.mlmodel")
4/99:
def factorial(n):
    """Return the factorial of n, an exact integer >= 0.

    >>> [factorial(n) for n in range(6)]
    [1, 1, 2, 6, 24, 120]
    >>> factorial(30)
    265252859812191058636308480000000
    >>> factorial(-1)
    Traceback (most recent call last):
        ...
    ValueError: n must be >= 0

    Factorials of floats are OK, but the float must be an exact integer:
    >>> factorial(30.1)
    Traceback (most recent call last):
        ...
    ValueError: n must be exact integer
    >>> factorial(30.0)
    265252859812191058636308480000000

    It must also not be ridiculously large:
    >>> factorial(1e100)
    Traceback (most recent call last):
        ...
    OverflowError: n too large
    """

    import math
    if not n >= 0:
        raise ValueError("n must be >= 0")
    if math.floor(n) != n:
        raise ValueError("n must be exact integer")
    if n+1 == n:  # catch a value like 1e300
        raise OverflowError("n too large")
    result = 1
    factor = 2
    while factor <= n:
        result *= factor
        factor += 1
    return result


if __name__ == "__main__":
    import doctest
    doctest.testmod()
 5/1:
# What version of Python do you have?
import sys

import tensorflow.keras
import pandas as pd
import sklearn as sk
import tensorflow as tf
import platform

print(f"Python Platform: {platform.platform()}")
print(f"Tensor Flow Version: {tf.__version__}")
print(f"Keras Version: {tensorflow.keras.__version__}")
print()
print(f"Python {sys.version}")
print(f"Pandas {pd.__version__}")
print(f"Scikit-Learn {sk.__version__}")
gpu = len(tf.config.list_physical_devices('GPU'))>0
print("GPU is", "available" if gpu else "NOT AVAILABLE")
 7/1:
print('PyDev console: using IPython 8.7.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['/Users/yuxuanyang/Downloads/GRU4Rec_TensorFlow-master'])
 8/1: import numpy
 9/1:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
10/1:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
10/2:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
10/3:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
10/4:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
10/5:
# my credential file
#key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-f2e09e85c3b3.json"
key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-7129911936a3.json"

credentials = service_account.Credentials.from_service_account_file(
    key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"],
)

client = bigquery.Client(credentials=credentials, project=credentials.project_id)
10/6:
#从bigquery中调取数据 用户的使用的access log和app的情报
class get_info_from_bigquery(object):
    def access_log(self, table_name):
        query = '\n    SELECT * FROM `encoded-adviser-131008.yang_20220414.{}` \n' .format(table_name)
        query_job = client.query(query)  # Make an API request.
        table = query_job.to_dataframe().sort_values(by=['user_id'])
        setattr(self, '{}'.format(table_name),table)
        #return table
    def original_table(self, table_name):
        query = '\n    SELECT * FROM `encoded-adviser-131008.yang_20220414.{}` \n' .format(table_name)
        query_job = client.query(query)  # Make an API request.
        table = query_job.to_dataframe()
        setattr(self, '{}'.format(table_name),table)
10/7:
bigquery_class = get_info_from_bigquery()
#bigquery_class.access_log('200user_genre_datetime_accesscount')
10/8: bigquery_class.access_log('2000user_app_datetime_accesscount')
10/9: bigquery_class.access_log('2000user_app_datetime_accesscount')
10/10: bigquery_class.access_log('2000user_app_datetime_accesscount')
10/11:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
10/12: bigquery_class.access_log('2000user_app_datetime_accesscount')
10/13: bigquery_class.access_log('2000user_app_datetime_accesscount')
10/14:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
10/15: bigquery_class.access_log('2000user_app_datetime_accesscount')
11/1:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import os
11/2:
from calendar import monthrange
def getData(year, month):
    num_days = monthrange(year, month)[1]
    from_date = str(month) + '/1/' + str(year)
    to_date = str(month) + '/' + str(num_days) + '/' + str(year)
    return [datetime.strftime(i,'%Y-%m-%d') for i in list(pd.date_range(from_date, to_date))]

getData(2020, 2)
11/3:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import os
11/4:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import os
11/5:
from calendar import monthrange
def getData(year, month):
    num_days = monthrange(year, month)[1]
    from_date = str(month) + '/1/' + str(year)
    to_date = str(month) + '/' + str(num_days) + '/' + str(year)
    return [datetime.strftime(i,'%Y-%m-%d') for i in list(pd.date_range(from_date, to_date))]

getData(2020, 2)
11/6:
# my credential file
#key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-f2e09e85c3b3.json"
def activate_bigquery_credential():
    key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-7129911936a3.json"
    credentials = service_account.Credentials.from_service_account_file(
        key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"],
    )
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)
activate_bigquery_credential()
11/7:
key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-7129911936a3.json"
credentials = service_account.Credentials.from_service_account_file(
     key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"],
    )
client = bigquery.Client(credentials=credentials, project=credentials.project_id)
11/8:
## select 1% continuous users as the sample users by random seed 10
def create_sample_user_list():
    import random 
    random.seed(10)
    randlist=[random.randint(0,len(user_201909)) for _ in range(len(user_201909)//10)]
    sample_users_201909=user_201909.iloc[randlist,:]
    sample_users_list_201909=sample_users_201909['user_id'].tolist()
    ## sample users dataframe to csv
    sample_users_201909.to_csv('/Users/yuxuanyang/Desktop/sample_user_201909.csv')
create_sample_user_list()
11/9:
##the users have continuous usage days during April 
def sample_user_201909log():
    query= """
        select *
    from `encoded-adviser-131008.yang_20211126.sample_user_201909log`
    """
    query_job = client.query(query)  # Make an API request.
    return query_job.to_dataframe()
sample_user_201909log=sorted(sample_user_201909log()['user_id'].unique())
12/1:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
12/2:
# my credential file
#key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-f2e09e85c3b3.json"
key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-7129911936a3.json"

credentials = service_account.Credentials.from_service_account_file(
    key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"],
)

client = bigquery.Client(credentials=credentials, project=credentials.project_id)
12/3:
#从bigquery中调取数据 用户的使用的access log和app的情报
class get_info_from_bigquery(object):
    def access_log(self, table_name):
        query = '\n    SELECT * FROM `encoded-adviser-131008.yang_20220414.{}` \n' .format(table_name)
        query_job = client.query(query)  # Make an API request.
        table = query_job.to_dataframe().sort_values(by=['user_id'])
        setattr(self, '{}'.format(table_name),table)
        #return table
    def original_table(self, table_name):
        query = '\n    SELECT * FROM `encoded-adviser-131008.yang_20220414.{}` \n' .format(table_name)
        query_job = client.query(query)  # Make an API request.
        table = query_job.to_dataframe()
        setattr(self, '{}'.format(table_name),table)
12/4:
bigquery_class = get_info_from_bigquery()
#bigquery_class.access_log('200user_genre_datetime_accesscount')
12/5: bigquery_class.access_log('2000user_app_datetime_accesscount')
13/1:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/2:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/3:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/4: samples_data.head(2).append(samples_data.tail(2))
13/5:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/6:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/7:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/8:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/9:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/10:
"""模型编译"""
model.compile(loss=binary_crossentropy, optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])
13/11:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/12:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
from tensorflow.python.keras.optimizers import Adam
#from tensorflow.keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/13:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/14:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/15:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/16: samples_data.head(2).append(samples_data.tail(2))
13/17:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/18:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/19:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/20:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/21:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/22:
"""模型编译"""
model.compile(loss=binary_crossentropy, optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])
13/23:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/24:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/25:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/26:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/27: samples_data.head(2).append(samples_data.tail(2))
13/28:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/29:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/30:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/31:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/32:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/33:
"""模型编译"""
model.compile(loss=binary_crossentropy, optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])
13/34:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/35:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/36:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/37:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/38: samples_data.head(2).append(samples_data.tail(2))
13/39:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/40:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/41:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/42:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/43:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/44:
"""模型编译"""
model.compile(loss=binary_crossentropy, metrics=[AUC()])
#optimizer=Adam(learning_rate=learning_rate)
13/45:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/46:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/47:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/48:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/49: samples_data.head(2).append(samples_data.tail(2))
13/50:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/51:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/52:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/53:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/54:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/55:
"""模型编译"""
model.compile(loss=binary_crossentropy, metrics=[AUC()])
#optimizer=Adam(learning_rate=learning_rate)
13/56:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/57:
"""可视化下看看训练情况"""
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
13/58:
from tensorflow import keras
keras.utils.plot_model(model, to_file='./DSIN.png', show_shapes=True)
13/59:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/60:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/61:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/62: samples_data.head(2).append(samples_data.tail(2))
13/63:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/64:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/65:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/66:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/67:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/68:
"""模型编译"""
model.compile(loss=binary_crossentropy,optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])
#
13/69:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/70:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizer_v2  import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/71:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizers_v2  import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/72:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizer_v2.adam import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/73:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from tensorflow.keras.optimizer_v2.adam import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/74:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/75:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/76:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/77:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/78: samples_data.head(2).append(samples_data.tail(2))
13/79:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/80:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/81:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/82:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/83:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/84:
"""模型编译"""
model.compile(loss=binary_crossentropy,optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])
#
13/85:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/86:
"""可视化下看看训练情况"""
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
13/87:
from tensorflow import keras
keras.utils.plot_model(model, to_file='./DSIN.png', show_shapes=True)
13/88:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/89:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/90:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/91: samples_data.head(2).append(samples_data.tail(2))
13/92:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/93:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/94:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/95:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/96:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/97:
"""模型编译"""
model.compile(loss=binary_crossentropy,optimizer=Adam(learning_rate=learning_rate), metrics=[Recall(top_k=10)])
#
13/98:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC, Recall, Precision
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/99:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC, Recall, Precision
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/100:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/101:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/102: samples_data.head(2).append(samples_data.tail(2))
13/103:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/104:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/105:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/106:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/107:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/108:
"""模型编译"""
model.compile(loss=binary_crossentropy,optimizer=Adam(learning_rate=learning_rate), metrics=[Recall(top_k=10)])
#
13/109:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
#from tensorflow.keras.metrics import AUC, Recall, Precision
import keras_metrics
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/110:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
#from tensorflow.keras.metrics import AUC, Recall, Precision
import keras_metrics
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/111:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
#from tensorflow.keras.metrics import AUC, Recall, Precision
import keras
import keras_metrics
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/112:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC, Recall, Precision
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/113:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/114:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/115: samples_data.head(2).append(samples_data.tail(2))
13/116:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/117:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/118:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/119:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/120:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/121:
"""模型编译"""
model.compile(loss=binary_crossentropy,optimizer=Adam(learning_rate=learning_rate), metrics=[Recall()])
#
13/122:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/123:
"""可视化下看看训练情况"""
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
13/124:
from tensorflow import keras
keras.utils.plot_model(model, to_file='./DSIN.png', show_shapes=True)
13/125:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC, Recall, Precision
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/126:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/127:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/128: samples_data.head(2).append(samples_data.tail(2))
13/129:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/130:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/131:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/132:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/133:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/134:
"""模型编译"""
def nll1(y_true, y_pred):
    """ Negative log likelihood. """

    # keras.losses.binary_crossentropy give the mean
    # over the last axis. we require the sum
    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)
model.compile(loss=nll1, optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])
#
13/135:
# python基础包
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# 特征处理与数据集划分
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from utils import DenseFeat, SparseFeat, VarLenSparseFeat

# 导入模型
from DSIN import DSIN

# 模型训练相关
import tensorflow as tf
from keras import backend as K
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.metrics import AUC, Recall, Precision
from tensorflow.keras.losses import binary_crossentropy
#from tensorflow.python.keras.optimizers import Adam
from keras.optimizers import Adam

# 一些相关设置
import warnings
plt.style.use('fivethirtyeight')
warnings.filterwarnings('ignore')
13/136:
"""读取数据"""
samples_data = pd.read_csv("data/movie_sample.txt", sep="\t", header = None)
samples_data.columns = ["user_id", "gender", "age", "hist_movie_id", "hist_len", "movie_id", "movie_type_id", "label"]

#  把历史行为序列转成整数列表
def str2list(x):
    return [int(i) for i in x.split(',')]
samples_data['hist_movie_id'] = samples_data['hist_movie_id'].apply(lambda x: str2list(x))
13/137:
"""会话处理"""
# 把hist_movie_id拆成6个会话， 每个会话的长度是10， 会话不够的0填充
for i in range(5):
    samples_data['sess' + str(i+1)] = samples_data['hist_movie_id'].apply(lambda x: x[i*10:(i+1)*10])

# 每个样本的会话个数  这里的hist_len其实就是sess_nums了，这里这么写是想练一波python处理
sess_nums = np.array([len([int(i) for i in l if int(i) != 0]) // 10 + 1 for l in samples_data['hist_movie_id']])
sess_nums = np.array([i if i <= 5 else 5 for i in sess_nums])   # 这个是正好50个行为的这种，按照上面那个算会是6，但其实5个会话
sess_max_count = 5

del samples_data['hist_movie_id']
13/138: samples_data.head(2).append(samples_data.tail(2))
13/139:
"""数据集"""
X = samples_data.drop(columns='label')
y = samples_data["label"]

# 构建mask 是个列表，每个元素代表每个session里面序列的mask
mask = []
for i in range(sess_max_count):
    mask.append(
        np.array([len([k for k in l if k != 0]) for l in X['sess'+str(i+1)]])
    )
13/140:
"""构建DSIN模型的输入格式"""
X_train = {"user_id": np.array(X["user_id"]), \
        "gender": np.array(X["gender"]), \
        "age": np.array(X["age"]), \
        "movie_id": np.array(X["movie_id"]), \
        "movie_type_id": np.array(X["movie_type_id"]), \
        "hist_len": np.array(X["hist_len"]), \
        "sess1": np.stack(X['sess1']), \
        "sess2": np.stack(X['sess2']), \
        "sess3": np.stack(X['sess3']), \
        "sess4": np.stack(X['sess4']), \
        "sess5": np.stack(X['sess5']), \
        "seq_length1": mask[0], \
        "seq_length2": mask[1], \
        "seq_length3": mask[2], \
        "seq_length4": mask[3], \
        "seq_length5": mask[4], \
        "sess_length": sess_nums
        }

y_train = np.array(y)
13/141:
"""特征封装"""

feature_columns = [SparseFeat('user_id', max(samples_data["user_id"])+1, embedding_dim=8), 
                    SparseFeat('gender', max(samples_data["gender"])+1, embedding_dim=8), 
                    SparseFeat('age', max(samples_data["age"])+1, embedding_dim=8), 
                    SparseFeat('movie_id', max(samples_data["movie_id"])+1, embedding_dim=8),
                    SparseFeat('movie_type_id', max(samples_data["movie_type_id"])+1, embedding_dim=8),
                    DenseFeat('hist_len', 1)]

feature_columns += [VarLenSparseFeat('sess1', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length1'),
                    VarLenSparseFeat('sess2', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length2'), 
                    VarLenSparseFeat('sess3', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length3'), 
                    VarLenSparseFeat('sess4', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length4'), 
                    VarLenSparseFeat('sess5', vocabulary_size=max(samples_data["movie_id"])+1, embedding_dim=8, maxlen=10, length_name='seq_length5'), 
                   ]
feature_columns += ['sess_length']

# 行为特征列表，表示的是基础特征
sess_feature_list = ['movie_id']
13/142:
"""设置超参数"""
learning_rate = 0.001
batch_size = 64
epochs = 50
13/143:
"""构建DSIN模型"""
model = DSIN(feature_columns, sess_feature_list, bias_encoding=True)
model.summary()
13/144:
"""模型编译"""
def nll1(y_true, y_pred):
    """ Negative log likelihood. """

    # keras.losses.binary_crossentropy give the mean
    # over the last axis. we require the sum
    return K.sum(K.binary_crossentropy(y_true, y_pred), axis=-1)
model.compile(loss=nll1, optimizer=Adam(learning_rate=learning_rate), metrics=[AUC()])
#
13/145:
"""模型训练"""
callbacks = [
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),   # 早停
    ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.01, verbose=1)  # 调整学习率
]
history = model.fit(X_train, 
                    y_train, 
                    epochs=epochs, 
                    validation_split=0.2, 
                    batch_size=batch_size,
                    callbacks = callbacks
                   )
13/146:
"""可视化下看看训练情况"""
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()
13/147:
from tensorflow import keras
keras.utils.plot_model(model, to_file='./DSIN.png', show_shapes=True)
14/1:
# 将一整个月内的手机使用log转换成interval时长序列
class access_interval_from_log(object):
    def access_log_to_interval(self,month_access_log, user_id ):
        access_timestamp_table=month_access_log[month_access_log['user_id']==\
                               user_id].sort_values(by='datetime_minute').iloc[:,1]
        access_count_table=month_access_log[month_access_log['user_id']==\
                       user_id].sort_values(by='datetime_minute').iloc[:,2]
#用户某月的accesslog table
        setattr(self,'{}_access_timestamp_table'.format(user_id) ,access_timestamp_table)   
#每次access的时点
        access_timestamp_list=access_timestamp_table.tolist()   
#每次access的次数
        access_count_list=access_count_table.tolist()
#（第一次access和最后一次access之间的interval时长） 分钟数
        total_access_length=int((access_timestamp_list[-1]-access_timestamp_list[0]).total_seconds()/60)+1 
#每次access之后的interval时长的list
        access_duration_list=[]
        for i in range(len(access_timestamp_list)):
            duration= int((access_timestamp_list[i]-access_timestamp_list[0]).total_seconds()/60)  
            access_duration_list.append(duration)
#做出整个使用时长的0-1序列
        total_access_dummy_list=[0]*total_access_length   
        total_access_count_list=[0]*total_access_length
        for j in range(len(access_duration_list)):
#dummy变量：access的时点用1替换
            total_access_dummy_list[access_duration_list[j]]=1 
#数值变量：access的时长用count替换   
            total_access_count_list[access_duration_list[j]]=access_count_list[j]
        setattr(self,'{}_total_access_dummy_list'.format(user_id), total_access_dummy_list)
        setattr(self,'{}_total_access_count_list'.format(user_id), total_access_count_list)
#interval时长序列tuple
        access_interval_tuple = [(k, len(list(v))) for k, v in itertools.groupby(total_access_dummy_list)]   
        setattr(self, '{}_access_interval_tuple'.format(user_id), access_interval_tuple)
        access_interval_table=pd.DataFrame(access_interval_tuple)
        access_interval_table.columns=['use or not', 'duration']      #inteval是否使用，和时长
        access_interval_mean=access_interval_table[access_interval_table['use or not']==0]['duration'].mean()
#interval时长平均值 
        setattr(self, '{}_access_interval_mean'.format(user_id), access_interval_mean)    
        access_interval_median=access_interval_table[access_interval_table['use or not']==0]['duration'].median()
#interval中位数
        setattr(self, '{}_access_interval_median'.format(user_id), access_interval_median)    
        interval_length=access_interval_table[access_interval_table['use or not']==0]['duration'].tolist()+  \
                                        access_interval_table[access_interval_table['use or not']==1]['duration'].sum()*[0]
        interval_average=np.mean(np.array(interval_length))
        
        setattr(self, '{}_access_interval_average'.format(user_id), interval_average)
14/2:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import os
14/3:
%%time
#用access log 生成interal
# create the class of interval get from access log
#attribute includes: dummy list, access count list, average/ mean/ median interval length
%store -r user_id_list
%store -r user_datetime_201909

continous_user_interval_1909=access_interval_from_log()
for u in user_id_list[:5]:
    continous_user_interval_1909.access_log_to_interval(user_datetime_201909,u)
del user_id_list, user_datetime_201909
os.system('say "your program was finished"')
14/4:
%store -r user_datetime_201909
%store -r user_datetime_201910
%store -r user_datetime_201911
%store -r user_datetime_201912
%store -r user_datetime_202001
%store -r user_datetime_202002
%store -r user_datetime_202003
%store -r user_datetime_202004
%store -r user_datetime_202005
%store -r user_datetime_202006
%store -r user_datetime_202007
%store -r user_datetime_202008
%store -r user_datetime_202009
%store -r user_datetime_202010
%store -r user_datetime_202011
%store -r user_datetime_202012
%store -r user_datetime_202101
%store -r user_datetime_202102
%store -r user_datetime_202103
%store -r user_datetime_202104
%store -r user_datetime_202105
%store -r user_datetime_202106
%store -r user_datetime_202107
%store -r user_datetime_202108

%store -r user_time_acount_201909
%store -r user_time_acount_201910
%store -r user_time_acount_201911
%store -r user_time_acount_201912
%store -r user_time_acount_202001
%store -r user_time_acount_202002
%store -r user_time_acount_202003
%store -r user_time_acount_202004
%store -r user_time_acount_202005
%store -r user_time_acount_202006
%store -r user_time_acount_202007
%store -r user_time_acount_202008
%store -r user_time_acount_202009
%store -r user_time_acount_202010
%store -r user_time_acount_202011
%store -r user_time_acount_202012
%store -r user_time_acount_202101
%store -r user_time_acount_202102
%store -r user_time_acount_202103
%store -r user_time_acount_202104
%store -r user_time_acount_202105
%store -r user_time_acount_202106
%store -r user_time_acount_202107
%store -r user_time_acount_202108

%store -r user_id_list
14/5:
# my credential file
#key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-f2e09e85c3b3.json"
def activate_bigquery_credential():
    key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-7129911936a3.json"
    credentials = service_account.Credentials.from_service_account_file(
        key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"],
    )
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)
activate_bigquery_credential()
14/6:
key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-7129911936a3.json"
credentials = service_account.Credentials.from_service_account_file(
     key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"],
    )
client = bigquery.Client(credentials=credentials, project=credentials.project_id)
14/7:
## select 1% continuous users as the sample users by random seed 10
def create_sample_user_list():
    import random 
    random.seed(10)
    randlist=[random.randint(0,len(user_201909)) for _ in range(len(user_201909)//10)]
    sample_users_201909=user_201909.iloc[randlist,:]
    sample_users_list_201909=sample_users_201909['user_id'].tolist()
    ## sample users dataframe to csv
    sample_users_201909.to_csv('/Users/yuxuanyang/Desktop/sample_user_201909.csv')
create_sample_user_list()
14/8:
## select 1% continuous users as the sample users by random seed 10
def create_sample_user_list():
    import random 
    random.seed(10)
    randlist=[random.randint(0,len(user_201909)) for _ in range(len(user_201909)//10)]
    sample_users_201909=user_201909.iloc[randlist,:]
    sample_users_list_201909=sample_users_201909['user_id'].tolist()
    ## sample users dataframe to csv
    sample_users_201909.to_csv('/Users/yuxuanyang/Desktop/sample_user_201909.csv')
create_sample_user_list()
15/1:
import numpy as np
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account
import pyarrow
from pandas import Timestamp
import itertools
from datetime import datetime
import numpy as np
import scipy.stats as stats
import matplotlib.pyplot as plt 
from collections import Counter
from itertools import groupby
pd.set_option('display.max_rows', 500)
import matplotlib
import statistics 
import os
os.environ['KMP_DUPLICATE_LIB_OK']='True'
import scipy.io as sio
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
plt.style.use('seaborn-poster')
%matplotlib inline
15/2:
# my credential file
#key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-f2e09e85c3b3.json"
key_path = "/Users/yuxuanyang/Downloads/encoded-adviser-131008-7129911936a3.json"

credentials = service_account.Credentials.from_service_account_file(
    key_path, scopes=["https://www.googleapis.com/auth/cloud-platform"],
)

client = bigquery.Client(credentials=credentials, project=credentials.project_id)
15/3:
#从bigquery中调取数据 用户的使用的access log和app的情报
class get_info_from_bigquery(object):
    def access_log(self, table_name):
        query = '\n    SELECT * FROM `encoded-adviser-131008.yang_20220414.{}` \n' .format(table_name)
        query_job = client.query(query)  # Make an API request.
        table = query_job.to_dataframe().sort_values(by=['user_id'])
        setattr(self, '{}'.format(table_name),table)
        #return table
    def original_table(self, table_name):
        query = '\n    SELECT * FROM `encoded-adviser-131008.yang_20220414.{}` \n' .format(table_name)
        query_job = client.query(query)  # Make an API request.
        table = query_job.to_dataframe()
        setattr(self, '{}'.format(table_name),table)
15/4:
bigquery_class = get_info_from_bigquery()
#bigquery_class.access_log('200user_genre_datetime_accesscount')
15/5: bigquery_class.access_log('2000user_app_datetime_accesscount')
15/6: bigquery_class.original_table('2000user_app_genre')
15/7:
#得到用户的accesslog和app的信息
user2000_app_datetime_accesscount = getattr(bigquery_class, '2000user_app_datetime_accesscount')
user2000_app_genre = getattr(bigquery_class, '2000user_app_genre')
15/8:
%store user2000_app_datetime_accesscount
%store user2000_app_genre
15/9:
app_list = user200_app_datetime_accesscount[user200_app_datetime_accesscount['user_id'].isin(user_id_list1)]['app_id'].unique().tolist()
X=user200_app_genre[user200_app_genre['app_id'].isin(app_list)]['genre'].unique()
len(X)
15/10:
%%time 
user200_new_interval = new_access_interval_from_log()
#%store -r sample_user_10per_202108
user_id_list1 = getattr(bigquery_class, '2000user_{}_datetime_accesscount'.format('app'))['user_id'].unique()
for u in user_id_list1:
     user200_new_interval.access_log_to_new_interval('app', u)
os.system('say "your program was finished"')
# del user200_genre_datetime_accesscount
15/11:
# 将一段时间内手机使用log转换成interval时长序列 (连续启动之间的时间间隔)
# 主要目的是查看每个app的interval时间长度分布
class new_access_interval_from_log(object):
    def access_log_to_new_interval(self, app_or_genre, user_id):  
        access_log =  getattr(bigquery_class, '2000user_{}_datetime_accesscount'.format(app_or_genre))
        access_timestamp_table_ = access_log[access_log['user_id']== user_id].sort_values(by=['datetime'])
        app_list = access_timestamp_table_['{}_id'.format(app_or_genre)].iloc[:-1].unique()
        #某个用户的a使用过的applist
        setattr(self,'{}_{}_list'.format(user_id,app_or_genre) ,app_list)
        app_order_list = access_timestamp_table_.sort_values(by='datetime').loc[:,'{}_id'.format(app_or_genre)].tolist()
        timestamp_list = access_timestamp_table_.sort_values(by='datetime').loc[:,'datetime'].tolist()
        count_list = access_timestamp_table_.sort_values(by='datetime').loc[:,'access_count'].tolist()
        setattr(self,'{}_{}_order_list'.format(user_id, app_or_genre) ,app_order_list)  
        setattr(self,'{}_timestamp_list'.format(user_id) ,timestamp_list)
        setattr(self,'{}_count_list'.format(user_id) ,count_list)
        duration_list = [(timestamp_list[i] - timestamp_list[0]).total_seconds() for i in range(len(timestamp_list))]
        setattr(self, '{}_duration_list'.format(user_id), duration_list)
        total_interval_list = diff_between_after(duration_list)
        setattr(self,'{}_total_interval_list'.format(user_id), total_interval_list)
        app_interval_list = []
        l = len(total_interval_list)
        for a in np.unique(np.array(app_order_list[:-1])).tolist():
        #找到某个使用appA的时点，转化成序号
            idx_lst = [i for i,t in enumerate(app_order_list[:l]) if t == a]
        #找到app A的使用时点序号
            app_duration = np.array(duration_list)[idx_lst]
            app_count = np.array(count_list)[idx_lst]
        #找到属于app A的使用时间间隔
            app_interval = np.array(total_interval_list)[idx_lst]
        #ユーザがアプリAを起動する時の時点
            setattr(self, '{}_{}_access_duration_list'.format(user_id,a), app_duration.tolist())
        #ユーザがアプリAを起動する回数
            setattr(self, '{}_{}_access_count_list'.format(user_id,a), app_count.tolist())
        #ユーザがアプリAの使用时间间隔
            setattr(self, '{}_{}_interval_list'.format(user_id,a), app_interval.tolist())
            app_interval_list.append(app_interval)
        setattr(self, '{}_app_interval_list'.format(user_id), app_interval_list)
15/12:
app_list = user2000_app_datetime_accesscount[user2000_app_datetime_accesscount['user_id'].isin(user_id_list1)]['app_id'].unique().tolist()
X=user2000_app_genre[user2000_app_genre['app_id'].isin(app_list)]['genre'].unique()
len(X)
15/13:
%%time 
user200_new_interval = new_access_interval_from_log()
#%store -r sample_user_10per_202108
user_id_list1 = getattr(bigquery_class, '2000user_{}_datetime_accesscount'.format('app'))['user_id'].unique()
for u in user_id_list1:
     user200_new_interval.access_log_to_new_interval('app', u)
os.system('say "your program was finished"')
# del user200_genre_datetime_accesscount
15/14:
#后一个与前一个元素的差值
def diff_between_after(lst):
    diff_list=[]
    for i in range(len(lst)-1):
        diff=lst[i+1]-lst[i]
        diff_list.append(diff)   
    return diff_list
15/15:
def xstr(Xlist):
    new_Xlist = []
    for s in Xlist:
        if s is None:
            new_Xlist.append('NONE')
        else:
            new_Xlist.append(s)
    return new_Xlist
15/16:
gen_table = user200_app_genre 
gen_list = xstr(gen_table['genre'].unique())
all_ginterval=[]
for g in gen_list:
    gen_app_list = gen_table[gen_table['genre']==g]['app_id']
    alluser_ginterval = []
    for u in user_id_list1[:10]:
        user_app_list = getattr(user200_new_interval, '{}_app_list'.format(u))
        common_app_list =[i for i in gen_app_list if i in user_app_list]
        if len(common_app_list)==[]:
            ginterval=[]
        else:
            ginterval=[]
            for a in common_app_list:
                ginterval += getattr(user200_new_interval, '{}_{}_interval_list'.format(u,a))
        alluser_ginterval += ginterval
    all_ginterval.append(alluser_ginterval)
len(all_ginterval)
15/17:
%%time 
user200_new_interval = new_access_interval_from_log()
#%store -r sample_user_10per_202108
user_id_list1 = getattr(bigquery_class, '2000user_{}_datetime_accesscount'.format('app'))['user_id'].unique()
for u in user_id_list1:
     user200_new_interval.access_log_to_new_interval('app', u)
os.system('say "your program was finished"')
# del user200_genre_datetime_accesscount
15/18:
%%time 
user200_new_interval = new_access_interval_from_log()
#%store -r sample_user_10per_202108
user_id_list1 = getattr(bigquery_class, '2000user_{}_datetime_accesscount'.format('app'))['user_id'].unique()
for u in user_id_list1:
     user200_new_interval.access_log_to_new_interval('app', u)
os.system('say "your program was finished"')
# del user200_genre_datetime_accesscount
15/19:
access_log = getattr(bigquery_class, '2000user_app_datetime_accesscount')
access_log[access_log['user_id']== '10a7a46ce3d1e7c61549a9979b5b78f4662f4438'].sort_values(by=['datetime']) #.unique()
18/1:
import pandas as pd
import numpy as np
import pyarrow
from datetime import datetime
import scipy.stats as stats
from collections import Counter
from itertools import groupby
import statistics
import os
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
20/1:
import pandas as pd
import numpy as np
import pyarrow
from datetime import datetime
import scipy.stats as stats
from collections import Counter
from itertools import groupby
import statistics
import os
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
FRAC = 0.25
20/2:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
20/3: print(user)
20/4: print(sample)
20/5:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and \
    os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:   # 随机抽取0.25的用户
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
                    str(FRAC) + '.pkl')
    pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
                    str(FRAC) + '.pkl')
20/6: print(sample_sub)
20/7: print(user_sub)
20/8: print(len(user_sub)/len(user))
20/9:
print(len(user_sub)/len(user))
print(len(sample_sub)/len(sample))
20/10: print(user_sub.userid.unique())
20/11: print(len(user_sub.userid.unique()))
20/12:
print(len(user_sub.userid))
print(len(user_sub.userid.unique()))
20/13:
print(len(user.userid))
print(len(user.userid.unique()))
20/14:
print(len(sample.user))
print(len(sample.user.unique()))
20/15:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
    log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
else:
    log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
    log = log.loc[log['btag'] == 'pv']
    pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
20/16:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
    log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
else:
    log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
    log = log.loc[log['btag'] == 'pv']
    pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
21/1:
# if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
#     log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
log = log.loc[log['btag'] == 'pv']
pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
21/2:
# if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
#     log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
log = log.loc[log['btag'] == 'pv']
pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
21/3:
import pandas as pd
import numpy as np
import pyarrow
from datetime import datetime
import scipy.stats as stats
from collections import Counter
from itertools import groupby
import statistics
import os
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import LabelEncoder
FRAC = 0.25
21/4:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
21/5:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and \
    os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:   # 随机抽取0.25的用户
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
                    str(FRAC) + '.pkl')
    pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
                    str(FRAC) + '.pkl')
21/6:
# if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
#     log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
log = log.loc[log['btag'] == 'pv']
pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
22/1:
import pandas as pd
import numpy as np
import pyarrow
from datetime import datetime
import scipy.stats as stats
from collections import Counter
from itertools import groupby
import statistics
import os
from scipy import fft, arange, signal
from scipy.stats import wasserstein_distance
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import LabelEncoder
FRAC = 0.25
22/2:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
22/3:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and \
    os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:   # 随机抽取0.25的用户
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
                    str(FRAC) + '.pkl')
    pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
                    str(FRAC) + '.pkl')
22/4:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
    log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
22/5: print(log)
22/6:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
    log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
22/7: print(log)
22/8:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
    log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
22/9: print(log)
22/10:
userset = user_sub.userid.unique()  # 抽取出的0.25的用户
log = log.loc[log.user.isin(userset)]  # 0.25用户的log
pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
unique_brand = np.ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/11:
userset = user_sub.userid.unique()  # 抽取出的0.25的用户
log = log.loc[log.user.isin(userset)]  # 0.25用户的log
#pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
unique_brand = np.ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/12:
userset = user_sub.userid.unique()  # 抽取出的0.25的用户
log = log.loc[log.user.isin(userset)]  # 0.25用户的log
#pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')
22/13:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/14:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/15:
lbe = LabelEncoder()
unique_brand = np.ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/16:


lbe = LabelEncoder()
unique_brand = ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/17: print(unique_cate_id)
22/18: unique_cate_id
22/19: len(unique_cate_id)
22/20: len(ad['cate_id'].unique())
22/21: len(log['cate'].unique())
22/22: 6741+6769
22/23:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/24: len(unique_cate_id)
22/25: ad['cate_id'].unique()
22/26: log['cate'].unique()
22/27: len(unique_cate_id)
22/28:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/29: unique_cate_id
22/30: unique_cate_id.unique()
22/31: unique_cate_id
22/32: liet(unique_cate_id)
22/33: list(unique_cate_id)
22/34: set(list(unique_cate_id))
22/35: len(set(list(unique_cate_id)))
22/36:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量
print(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/37: ad
22/38:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
print(ad)
print(log)
22/39:
print(ad)
print(log)
22/40:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
print(ad)
print(log)
22/41:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量
print(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/42:
print(ad)
print(log)
22/43:
unique_cate_id = np.concatenate(ad['cate_id'].unique())
#, log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量
print(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/44:
unique_cate_id = ad['cate_id'].unique()
#, log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量
print(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/45:
print(ad)
print(log)
22/46:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/47:
unique_cate_id = ad['cate_id'].unique()
#, log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/48:
len(unique_brand)
len(log['brand'].unique())
22/49:
print(len(unique_brand))
print(len(log['brand'].unique()))
22/50:
print(ad)
print(log)
22/51:
lbe = LabelEncoder()
unique_brand = ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = ad['brand'].unique()
#, log['brand'].unique())

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/52:
print(ad)
print(log)
22/53:
unique_cate_id = ad['cate_id'].unique()
#, log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/54:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/55:
unique_cate_id = ad['cate_id'].unique()
#, log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/56:
unique_cate_id = np.concatenate(ad['cate_id'].unique(), log['cate'].unique())
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/57:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/58:
unique_cate_id = np.concatenate(ad['cate_id'].unique(), log['cate'].unique())
lbe.fit(unique_cate_id) # 生成cate的向量
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/59:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量
print(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
22/60:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(cate)
22/61:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/62:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
print(unique_cate_id)
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/63:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/64:
lbe = LabelEncoder()
unique_brand = ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/65:
print(ad)
print(log)
22/66:
lbe = LabelEncoder()
unique_brand = ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(ad['brand'].unique())
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/67:
print(ad)
print(log)
22/68:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(ad['cate_id'].unique()) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/69:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/70:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(ad['cate_id'].unique()) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/71:
lbe = LabelEncoder()
unique_brand = ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(ad['brand'].unique())
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/72:
print(ad)
print(log)
22/73:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/74:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/75:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/76:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
    log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
22/77:
userset = user_sub.userid.unique()  # 抽取出的0.25的用户
log = log.loc[log.user.isin(userset)]  # 0.25用户的log
#pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')
22/78:
ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')  # 所有广告的信息
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
unique_cate_id = ad['cate_id'].unique()
log = log.loc[log.cate.isin(unique_cate_id)]
22/79:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/80:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(ad['cate_id'].unique()) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/81:
unique_cate_id = np.concatenate((ad['cate_id'].unique(), log['cate'].unique()))
lbe.fit(unique_cate_id) # 生成cate的向量

ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1
print(ad)
print(log)
22/82:
lbe = LabelEncoder()
unique_brand = ad['brand'].unique()
log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate((ad['brand'].unique(), log['brand'].unique()))

lbe.fit(ad['brand'].unique())
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
22/83:
print(ad)
print(log)
22/84:
import gc

import pandas as pd
from joblib import Parallel, delayed

import random
22/85: log
22/86: log.user.unique()
22/87: len(log.user.unique())
22/88: log.user.unique()
22/89: log[log.user == '857237']
22/90: log[log.user == 857237]
22/91: print(log.time_stamp.sort())
22/92: print(log.time_stamp.tolist().sort())
24/1:
import os

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
FRAC = 0.25
24/2:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
24/3:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
24/4:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
    else:
        log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
        log = log.loc[log['btag'] == 'pv']
        pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
24/6:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
else:
    log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
    log = log.loc[log['btag'] == 'pv']
    pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
24/7:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
24/8: log
24/9:
log = log.loc[log.user.isin(sample_sub.user.unique())]
log.drop(columns=['btag'], inplace=True)
log = log.loc[log['time_stamp'] > 0]

# pd.to_pickle(ad, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/ad_feature_enc_' + str(FRAC) + '.pkl')
# pd.to_pickle(
#     log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')

# print("0_gen_sampled_data done")
log
24/10:
log = log.loc[log.user.isin(sample_sub.user.unique())]
log.drop(columns=['btag'], inplace=True)
#log = log.loc[log['time_stamp'] > 0]

# pd.to_pickle(ad, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/ad_feature_enc_' + str(FRAC) + '.pkl')
# pd.to_pickle(
#     log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')

# print("0_gen_sampled_data done")
log
24/11:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
24/12:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
24/13: log
24/14: log
24/15: log
24/16: log
24/17:
log = log.loc[log.user.isin(sample_sub.user.unique())]
log.drop(columns=['btag'], inplace=True)
#log = log.loc[log['time_stamp'] > 0]

# pd.to_pickle(ad, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/ad_feature_enc_' + str(FRAC) + '.pkl')
# pd.to_pickle(
#     log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')

# print("0_gen_sampled_data done")
log
24/18: log.loc[log['time_stamp'] <= 0]
24/19:
log = log.loc[log['time_stamp'] > 0]
log
24/20: log.user.unique()
24/21: len(log.user.unique())
24/22: len(log.cate.unique())
24/23: len(log.brand.unique())
24/24:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_diff = sorted_log.time
24/25:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_diff = sorted_log.time_stamp[1:] - sorted_log.time_stamp[:-1]
time_diff
24/26:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_diff = sorted_log.time_stamp[1:] - sorted_log.time_stamp[:-1]
sorted_log.time_stamp
24/27:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_diff = sorted_log.time_stamp[1:] - sorted_log.time_stamp[:-1]
np.array(sorted_log.time_stamp)
24/28:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_array = np.array(sorted_log.time_stamp)
time_diff = time_array[1:] - time_array[:-1]
time_diff
24/29:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_array = np.array(sorted_log.time_stamp)
time_diff = time_array[1:] - time_array[:-1]
time_diff /30
24/30:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_array = np.array(sorted_log.time_stamp)
time_diff = time_array[1:] - time_array[:-1]
time_diff
24/31:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_array = np.array(sorted_log.time_stamp)
time_diff = time_array[1:] - time_array[:-1]
zip(sorted_log.cate[:-1], time_diff)
24/32:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_array = np.array(sorted_log.time_stamp)
time_diff = time_array[1:] - time_array[:-1]
print(dict(zip(sorted_log.cate[:-1], time_diff)))
24/33: sorted_log.cate[:-1]
24/34:
keys = ['a', 'b', 'c']
values = [1, 2, 3]
dictionary = dict(zip(keys, values))
dictionary['A']
24/35:
keys = ['a', 'b', 'c']
values = [1, 2, 3]
dictionary = dict(zip(keys, values))
dictionary['b']
24/36:
keys = ['a', 'b', 'c']
values = [1, 2, 3]
dictionary = dict(zip(keys, values))
dictionary.keys
24/37:
keys = ['a', 'b', 'c']
values = [1, 2, 3]
dictionary = dict(zip(keys, values))
dictionary.keys()
24/38:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_array = np.array(sorted_log.time_stamp)
time_diff = time_array[1:] - time_array[:-1]
cate_interval = {}
for i in range(len(time_diff)):
    if sorted_log.cate[i] not in cate_interval.keys():
        cate_interval[sorted_log.cate[i]] = []
    cate_interval[sorted_log.cate[i]].append(time_diff[i])
24/39:
user_log = log[log.user == 857237]
sorted_log = user_log.sort_values(by =['time_stamp'])
time_array = np.array(sorted_log.time_stamp)
time_diff = time_array[1:] - time_array[:-1]
cate_interval = {}
for i in range(len(time_diff)):
    if sorted_log.cate.tolist()[i] not in cate_interval.keys():
        cate_interval[sorted_log.cate.tolist()[i]] = []
    cate_interval[sorted_log.cate.tolist()[i]].append(time_diff[i])
24/40: cate_interval
24/41:
def gen_user_interval_list(user_id, log):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return [cate_list, time_diff.tolist()]

cate_interval = {}
def gen_cate_interval_dict(log, cate_interval):
    user_list = log.user.unique().tolist()
    for u in user_list:
        cate_list, time_diff = gen_user_interval_list(u,log)
        for i in range(len(time_diff)):
            if cate_list[i] not in cate_interval.keys():
                cate_interval[cate_list[i]] = []
            cate_interval[cate_list[i]].append(time_diff[i])
    return cate_interval
cate_interval_dict = gen_cate_interval_dict(log, cate_interval)
cate_interval_dict
24/42: user_sub
24/43: sample_sub
24/44: from tqdm import tqdm
24/45:
def gen_user_interval_list(user_id, log):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return [cate_list, time_diff.tolist()]

cate_interval = {}
def gen_cate_interval_dict(log, cate_interval):
    user_list = log.user.unique().tolist()
    for u in tqdm(user_list):
        cate_list, time_diff = gen_user_interval_list(u,log)
        for i in range(len(time_diff)):
            if cate_list[i] not in cate_interval.keys():
                cate_interval[cate_list[i]] = []
            cate_interval[cate_list[i]].append(time_diff[i])
    return cate_interval
cate_interval_dict = gen_cate_interval_dict(log, cate_interval)
cate_interval_dict
24/46:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
Parallel(n_jobs = -1)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in tqdm(user_list))

cate_interval
24/47:
from tqdm import tqdm
from joblib import Parallel, delayed
24/48:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
Parallel(n_jobs = -1)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in tqdm(user_list))

cate_interval
24/49:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in user_list:
    gen_user_interval_list(u,log,cate_interval)

#Parallel(n_jobs = -1)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in tqdm(user_list))
cate_interval
24/50:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)

#Parallel(n_jobs = -1)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in tqdm(user_list))
cate_interval
24/51:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return cate_list, time_diff
    # for i in range(len(time_diff)):
    #     if cate_list[i] not in cate_interval.keys():
    #         cate_interval[cate_list[i]] = []
    #     cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
# for u in tqdm(user_list):
#     gen_user_interval_list(u,log,cate_interval)

result = Parallel(n_jobs = -2)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in tqdm(user_list))
result
24/52:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return cate_list, time_diff
    # for i in range(len(time_diff)):
    #     if cate_list[i] not in cate_interval.keys():
    #         cate_interval[cate_list[i]] = []
    #     cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
# for u in tqdm(user_list):
#     gen_user_interval_list(u,log,cate_interval)

result = Parallel(n_jobs = -2)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
result
24/53:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return cate_list, time_diff
    # for i in range(len(time_diff)):
    #     if cate_list[i] not in cate_interval.keys():
    #         cate_interval[cate_list[i]] = []
    #     cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
# for u in tqdm(user_list):
#     gen_user_interval_list(u,log,cate_interval)

result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
result
24/54:
@delayed
def compute_something(i: int):
    return i ** 2

results = Parallel(n_jobs=10)(
    compute_something(i) for i in range(100)
)
24/55:
with tqdm_joblib(100):
    results = Parallel(n_jobs=10)(
        compute_something(i) for i in range(100)
    )
24/56:
import contextlib
from typing import Optional
import joblib
from tqdm.auto import tqdm

@contextlib.contextmanager
def tqdm_joblib(total: Optional[int] = None, **kwargs):

    pbar = tqdm(total=total, miniters=1, smoothing=0, **kwargs)

    class TqdmBatchCompletionCallback(joblib.parallel.BatchCompletionCallBack):
        def __call__(self, *args, **kwargs):
            pbar.update(n=self.batch_size)
            return super().__call__(*args, **kwargs)

    old_batch_callback = joblib.parallel.BatchCompletionCallBack
    joblib.parallel.BatchCompletionCallBack = TqdmBatchCompletionCallback

    try:
        yield pbar
    finally:
        joblib.parallel.BatchCompletionCallBack = old_batch_callback
        pbar.close()
24/57:
with tqdm_joblib(100):
    results = Parallel(n_jobs=10)(
        compute_something(i) for i in range(100)
    )
24/58:
with tqdm_joblib(100):
    results = Parallel(n_jobs=10)(
        compute_something(i) for i in range(100)
    )
24/59:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:100]
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
cate_interval
# result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
# result
24/60:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    #return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:100]
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
cate_interval
# result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
# result
24/61:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    #return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:100]
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
cate_interval
# result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
# result
24/62:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    #return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:100]
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
print(cate_interval)
# result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
# result
24/63:
from tqdm import tqdm
from joblib import Parallel, delayed
24/64:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    #return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:100]
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
print(cate_interval)
# result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
# result
24/65:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    #return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:1000]
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
cate_interval
# result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
# result
24/66:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return cate_list, time_diff
    # for i in range(len(time_diff)):
    #     if cate_list[i] not in cate_interval.keys():
    #         cate_interval[cate_list[i]] = []
    #     cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:1000]
# for u in tqdm(user_list):
#     gen_user_interval_list(u,log,cate_interval)
# cate_interval
result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
result
24/67:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
    return cate_list, time_diff
    # for i in range(len(time_diff)):
    #     if cate_list[i] not in cate_interval.keys():
    #         cate_interval[cate_list[i]] = []
    #     cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()[:100]
# for u in tqdm(user_list):
#     gen_user_interval_list(u,log,cate_interval)
# cate_interval
result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
result
24/68:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
# result = Parallel(n_jobs = 5)(delayed(gen_user_interval_list)(u,log,cate_interval) for u in user_list)
# result
24/69:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
24/70:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
24/71:
import os

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
FRAC = 0.25
24/72:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
24/73:
from tqdm import tqdm
from joblib import Parallel, delayed
24/74:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
25/1:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
25/2:
import os

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
FRAC = 0.25
25/3:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
25/4:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
25/5:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
25/6:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
25/7:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
25/8:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
25/9:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
25/10:
log = log.loc[log.user.isin(sample_sub.user.unique())]
log.drop(columns=['btag'], inplace=True)

# pd.to_pickle(ad, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/ad_feature_enc_' + str(FRAC) + '.pkl')
# pd.to_pickle(
#     log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')

# print("0_gen_sampled_data done")
log
25/11:
log = log.loc[log['time_stamp'] > 0]
log
25/12:
from tqdm import tqdm
from joblib import Parallel, delayed
25/13:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
26/1: cate_interval
26/2:
import os

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
FRAC = 0.25
26/3:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
26/4:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
26/5:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
26/6:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
26/7:
log = log.loc[log.user.isin(sample_sub.user.unique())]
log.drop(columns=['btag'], inplace=True)

# pd.to_pickle(ad, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/ad_feature_enc_' + str(FRAC) + '.pkl')
# pd.to_pickle(
#     log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')

# print("0_gen_sampled_data done")
log
26/8:
log = log.loc[log['time_stamp'] > 0]
log
26/9:
from tqdm import tqdm
from joblib import Parallel, delayed
26/10:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
26/11: cate_interval
26/12: len(cate_interval)
26/13: pd.to_pickle(cate_interval, '/Users/yuxuanyang/Downloads/DSIN-master/sample_data/sample_cate_interval.pkl')
26/14: pd.to_pickle(cate_interval, '/Users/yuxuanyang/Downloads/DSIN-master/sample_data/sampled_cate_interval.pkl')
26/15: pd.to_pickle(cate_interval, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/sampled_cate_interval.pkl')
26/16: len(cate_interval)
26/17:
mean_interval = []
for i in cate_interval.keys():
    intervals = cate_interval[i]
    mean_interval.append(round(sum(intervals) / len(intervals),2))
mean_interval
26/18: sorted(mean_interval)
26/19: sorted(mean_interval)[-1]
26/20:
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl
26/21: cate_interval.values()
27/1:
import os

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
27/2:
import os

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
27/3:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
27/4:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
27/5:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
27/6:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
27/7:
log = log.loc[log.user.isin(sample_sub.user.unique())]
log.drop(columns=['btag'], inplace=True)

# pd.to_pickle(ad, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/ad_feature_enc_' + str(FRAC) + '.pkl')
# pd.to_pickle(
#     log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')

# print("0_gen_sampled_data done")
log
27/8:
log = log.loc[log['time_stamp'] > 0]
log
27/9:
from tqdm import tqdm
from joblib import Parallel, delayed
27/10:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list):
    gen_user_interval_list(u,log,cate_interval)
28/1:  yoox = 880080
28/2: %store yoox
29/1: yoox
29/2: %store -r yoox
29/3: yoox
29/4:
ser = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
29/5:
import os

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
29/6:
ser = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
29/7:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
29/8:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
29/9:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
29/10:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
30/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
30/2: pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl').columns
30/3: pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')[:30]
30/4:
conn = sqlite3.connect('logs.db')
c = conn.cursor()

c.execute('''
CREATE TABLE log (user int, time_stamp int, btag text, cate int, brand int)
''')

log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
log.to_sql('log', conn, if_exists = 'append', index = False)
31/1: log[:1]
31/2:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
31/3:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
31/4:
%store user
%store sample
31/5:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
31/6:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
%store log
32/1: pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/sampled_cate_interval.pkl')
32/2:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
32/3: pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/sampled_cate_interval.pkl')
32/4:
user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
32/5:
if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
    os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
    user_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
    sample_sub = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
else:

    if FRAC < 1.0:
        user_sub = user.sample(frac=FRAC, random_state=1024)
    else:
        user_sub = user
    sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
    #                 str(FRAC) + '.pkl')
    # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
    #                 str(FRAC) + '.pkl')
32/6:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
%store log
32/7:
if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
# else:
#     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
#     log = log.loc[log['btag'] == 'pv']
#     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
32/8:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
32/9:
conn = sqlite3.connect('logs.db')
# c = conn.cursor()

# c.execute('''
# CREATE TABLE log (user int, time_stamp int, btag text, cate int, brand int)
# ''')

log.to_sql('log', conn, if_exists = 'append', index = False)
33/1:
import sqlite3
import numpy as np
import pandas as pd
33/2: conn = sqlite3.connect('logs.db')
33/3:
print(pd.read_sql('''
SELECT * FROM log LIMIT 20
''', conn))
33/4:
print(pd.read_sql('''
SELECT user FROM log 
''', conn))
34/1:
import sqlite3
import numpy as np
import pandas as pd
34/2: conn = sqlite3.connect('logs.db')
34/3:
print(pd.read_sql('''
SELECT DISTINCT user FROM log 
''', conn))
34/4:
users = pd.read_sql('''
SELECT DISTINCT user FROM log 
''', conn)
users[:10]
34/5:
pd.read_sql('''
SELECT * FROM log WHERE user = 857237
''', conn)
34/6:
pd.read_sql('''
SELECT time_stamp, cate FROM log WHERE user = 857237
''', conn)
34/7: a = 1
34/8:
pd.read_sql('''
SELECT time_stamp, cate FROM log WHERE user = {}
'''.format(a), conn)
34/9: a = 85663
34/10:
pd.read_sql('''
SELECT time_stamp, cate FROM log WHERE user = {}
'''.format(a), conn)
34/11:
pd.read_sql('''
SELECT time_stamp, cate FROM log 
limit 10
''', conn)
34/12:
pd.read_sql('''
SELECT time_stamp, cate FROM log WHERE user = {}
'''.format(a), conn)
34/13:
pd.read_sql('''
SELECT time_stamp, cate FROM log WHERE user = 972
''', conn)
34/14:
pd.read_sql('''
SELECT * FROM log limit 10
''', conn)
34/15: a = 857237
34/16:
pd.read_sql('''
SELECT * FROM log WHERE user = {}
'''.format(a), conn)
34/17:
log = pd.read_sql('''
SELECT * FROM log
''', conn)
32/10:
cate_interval = {}
user_list = pd.read_sql('''
SELECT DISTINCT user from log
''').tolist()
32/11:
cate_interval = {}
user_list = pd.read_sql('''
SELECT DISTINCT user from log
''', conn).tolist()
32/12:
cate_interval = {}
user_list = np.array(pd.read_sql('''
SELECT DISTINCT user from log
''', conn))
32/13: user_list = log.user.unique().tolist()
32/14:
cate_interval = {}
user_list = log.user.unique().tolist()
32/15:
def gen_user_interval_list(user_id, cate_interval):
    user_log = pd.read_sql('''
SELECT time_stamp, cate FROM log WHERE user = {} ORDER BY time_stamp
'''.format(user_id), conn)
    time_array = np.array(user_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = user_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
32/16:
def gen_user_interval_list(user_id, cate_interval):
    user_log = pd.read_sql('''
SELECT time_stamp, cate FROM log WHERE user = {} ORDER BY time_stamp
'''.format(user_id), conn)
    time_array = np.array(user_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = user_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u, cate_interval)
32/17: %store log
35/1:
import sqlite3
import numpy as np
import pandas as pd
35/2: sqlite3.sqlite_version
32/18:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])

user_list = log.user.unique().tolist()
for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
32/19:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log['user'] == user_id](by =['time_stamp'])
    sorted_log = user_log.sort_values
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
32/20:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.iloc['user'] == user_id](by =['time_stamp'])
    sorted_log = user_log.sort_values
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
32/21:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id](by =['time_stamp'])
    sorted_log = user_log.sort_values
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
32/22:
cate_interval = {}
user_list = np.array(log.user.unique())
32/23:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
32/24: 260000/100*8.8
32/25: 260000/100*8.8/60
32/26: 260000/100*8.8/60/60
32/27:
import time
import pandas as pd
from pandarallel import pandarallel # 导入pandaralle
 
pandarallel.initialize() # 初始化该这个b...并行库
 
def square(x):
    return x**2
 
nums=list(range(10000000))
nums_df=pd.DataFrame({"num":nums})
 
time_start=time.time()
nums_df["square"]=nums_df["num"].parallel_apply(square) # 计算数组所有数的平方,注意，用的是parallel_apply
time_end=time.time()
 
print("并行代码耗时 %f s"%(time_end-time_start))  # 并行代码耗时 1.465182 s
32/28:
import time
import pandas as pd
from pandarallel import pandarallel # 导入pandaralle
 
pandarallel.initialize() # 初始化该这个b...并行库
 
def square(x):
    return x**2
 
nums=list(range(10000000))
nums_df=pd.DataFrame({"num":nums})
 
time_start=time.time()
nums_df["square"]=nums_df["num"].parallel_apply(square) # 计算数组所有数的平方,注意，用的是parallel_apply
time_end=time.time()
 
print("并行代码耗时 %f s"%(time_end-time_start))  # 并行代码耗时 1.465182 s
32/29: log.groupby('user').iloc[:100]
32/30: log.groupby('user')[:100]
32/31: log.groupby('user')
32/32: log.groupby('user').mean()
32/33:
def beta_cal_mult(one_fund_df):
    ll = list()
    for ind in range(len(one_fund_df)):
        one_fund_df_sub = one_fund_df.iloc[ind:ind + 20]
        ll.append(cross_regression(one_fund_df_sub, ['NAV_ADJ_RETURN1'], ['bench_mark_return']).params['bench_mark_return'])

    one_fund_df['beta_mult'] = pd.Series(ll).shift(19).tolist()
    # print pd.Series(ll).shift(19)
    return one_fund_df
32/34:
import pandas as pd
from joblib import Parallel, delayed
import multiprocessing
import statsmodels.api as sm


def cross_regression(df_temp, y_name, x_list, constant=True):
    df = df_temp.dropna()
    y = df[y_name]
    x = df[x_list]
    X = sm.add_constant(x) if constant else x
    results = sm.OLS(y, X, hasconst=constant).fit()
    return results

def beta_cal_mult(one_fund_df):
    ll = list()
    for ind in range(len(one_fund_df)):
        one_fund_df_sub = one_fund_df.iloc[ind:ind + 20]
        ll.append(cross_regression(one_fund_df_sub, ['NAV_ADJ_RETURN1'], ['bench_mark_return']).params['bench_mark_return'])

    one_fund_df['beta_mult'] = pd.Series(ll).shift(19).tolist()
    print pd.Series(ll).shift(19)
    return one_fund_df

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)



data_df = pd.read_hdf('test.h5')
multi_res = applyParallel(data_df.iloc[:10000].groupby('code'), beta_cal_mult)
multi_res.to_hdf('fil.h5', key='data')
32/35:
import pandas as pd
from joblib import Parallel, delayed
import multiprocessing
import statsmodels.api as sm


def cross_regression(df_temp, y_name, x_list, constant=True):
    df = df_temp.dropna()
    y = df[y_name]
    x = df[x_list]
    X = sm.add_constant(x) if constant else x
    results = sm.OLS(y, X, hasconst=constant).fit()
    return results

def beta_cal_mult(one_fund_df):
    ll = list()
    for ind in range(len(one_fund_df)):
        one_fund_df_sub = one_fund_df.iloc[ind:ind + 20]
        ll.append(cross_regression(one_fund_df_sub, ['NAV_ADJ_RETURN1'], ['bench_mark_return']).params['bench_mark_return'])

    one_fund_df['beta_mult'] = pd.Series(ll).shift(19).tolist()
    print (pd.Series(ll).shift(19))
    return one_fund_df

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)



data_df = pd.read_hdf('test.h5')
multi_res = applyParallel(data_df.iloc[:10000].groupby('code'), beta_cal_mult)
multi_res.to_hdf('fil.h5', key='data')
32/36:
import pandas as pd
from joblib import Parallel, delayed
import multiprocessing
import statsmodels.api as sm


def cross_regression(df_temp, y_name, x_list, constant=True):
    df = df_temp.dropna()
    y = df[y_name]
    x = df[x_list]
    X = sm.add_constant(x) if constant else x
    results = sm.OLS(y, X, hasconst=constant).fit()
    return results

def beta_cal_mult(one_fund_df):
    ll = list()
    for ind in range(len(one_fund_df)):
        one_fund_df_sub = one_fund_df.iloc[ind:ind + 20]
        ll.append(cross_regression(one_fund_df_sub, ['NAV_ADJ_RETURN1'], ['bench_mark_return']).params['bench_mark_return'])

    one_fund_df['beta_mult'] = pd.Series(ll).shift(19).tolist()
    print (pd.Series(ll).shift(19))
    return one_fund_df

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)



data_df = pd.read_hdf('test.h5')
multi_res = applyParallel(data_df.iloc[:10000].groupby('code'), beta_cal_mult)
multi_res.to_hdf('fil.h5', key='data')
32/37:
import pandas as pd
from joblib import Parallel, delayed
import multiprocessing
import statsmodels.api as sm


def cross_regression(df_temp, y_name, x_list, constant=True):
    df = df_temp.dropna()
    y = df[y_name]
    x = df[x_list]
    X = sm.add_constant(x) if constant else x
    results = sm.OLS(y, X, hasconst=constant).fit()
    return results

def beta_cal_mult(one_fund_df):
    ll = list()
    for ind in range(len(one_fund_df)):
        one_fund_df_sub = one_fund_df.iloc[ind:ind + 20]
        ll.append(cross_regression(one_fund_df_sub, ['NAV_ADJ_RETURN1'], ['bench_mark_return']).params['bench_mark_return'])

    one_fund_df['beta_mult'] = pd.Series(ll).shift(19).tolist()
    print (pd.Series(ll).shift(19))
    return one_fund_df

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)



data_df = log
multi_res = applyParallel(data_df.iloc[:10000].groupby('user'), beta_cal_mult)
#multi_res.to_hdf('fil.h5', key='data')
32/38:
log.sort_values(['user', 'time_stamp'], inplace=True)
def xf(d):
    print(d)
log.groupby('user').apply(xf)
36/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
36/2:
def gen_sampled_log():
    user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
    sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
    if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
        os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

    if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
        user_sub = pd.read_pickle(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
        sample_sub = pd.read_pickle(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
    else:

        if FRAC < 1.0:
            user_sub = user.sample(frac=FRAC, random_state=1024)
        else:
            user_sub = user
        sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
        # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
        #                 str(FRAC) + '.pkl')
        # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
        #                 str(FRAC) + '.pkl')
    if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
    # else:
    #     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
    #     log = log.loc[log['btag'] == 'pv']
    #     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
    return log
36/3:
def gen_sampled_log():
    user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
    sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
    if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
        os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

    if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
        user_sub = pd.read_pickle(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
        sample_sub = pd.read_pickle(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
    else:

        if FRAC < 1.0:
            user_sub = user.sample(frac=FRAC, random_state=1024)
        else:
            user_sub = user
        sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
        # pd.to_pickle(user_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' +
        #                 str(FRAC) + '.pkl')
        # pd.to_pickle(sample_sub, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' +
        #                 str(FRAC) + '.pkl')
    if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl'):
        log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
    # else:
    #     log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log.csv')
    #     log = log.loc[log['btag'] == 'pv']
    #     pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
    return log
36/4: log = gen_sampled_log()
36/5: log = gen_sampled_log()
36/6: sampled_log = gen_sampled_log()
36/7: log
36/8:
def gen_sampled_log():
    user = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/user_profile.csv')
    sample = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/raw_sample.csv')
    if not os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'):
        os.mkdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/')

    if os.path.exists('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl') and os.path.exists(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl'):
        user_sub = pd.read_pickle(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_profile_' + str(FRAC) + '_.pkl')
        sample_sub = pd.read_pickle(
            '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/raw_sample_' + str(FRAC) + '_.pkl')
    else:

        if FRAC < 1.0:
            user_sub = user.sample(frac=FRAC, random_state=1024)
        else:
            user_sub = user
        sample_sub = sample.loc[sample.user.isin(user_sub.userid.unique())]
    log = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/behavior_log_pv.pkl')
    return log
36/9: sampled_log = gen_sampled_log()
36/10: %store -r log
37/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
37/2: %store -r log
37/3:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
37/4:
%store user
%store sample
37/5: %store -r log
37/6:
userset = user_sub.userid.unique()
log = log.loc[log.user.isin(userset)]
# pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_' + str(FRAC) + '_.pkl')

ad = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/raw_data/ad_feature.csv')
ad['brand'] = ad['brand'].fillna(-1)

lbe = LabelEncoder()
# unique_cate_id = ad['cate_id'].unique()
# log = log.loc[log.cate.isin(unique_cate_id)]

unique_cate_id = np.concatenate(
    (ad['cate_id'].unique(), log['cate'].unique()))

lbe.fit(unique_cate_id)
ad['cate_id'] = lbe.transform(ad['cate_id']) + 1
log['cate'] = lbe.transform(log['cate']) + 1

lbe = LabelEncoder()
# unique_brand = np.ad['brand'].unique()
# log = log.loc[log.brand.isin(unique_brand)]

unique_brand = np.concatenate(
    (ad['brand'].unique(), log['brand'].unique()))

lbe.fit(unique_brand)
ad['brand'] = lbe.transform(ad['brand']) + 1
log['brand'] = lbe.transform(log['brand']) + 1
37/7:
conn = sqlite3.connect('logs.db')
log = pd.read_sql(''' SELECT * FROM log ''', conn)
38/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
38/2:
conn = sqlite3.connect('logs.db')
log = pd.read_sql(''' SELECT * FROM log ''', conn)
38/3: log.to_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')
38/4: log
38/5: log
38/6: log[:10]
38/7: pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')[:10]
38/8: print(pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')[:10])
39/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
FRAC = 0.25
39/2: print(pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')[:10])
39/3:
log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')
log[:10]
39/4:
cate_interval = {}
user_list = np.array(log.user.unique())
39/5:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
39/6: log.iloc[1:,:]
39/7: log.columns
39/8: log.drop(columns = ['Unnamed: 0', 'btag',  'brand'])
39/9: log.to_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')
39/10: log
39/11: log = log.drop(columns = ['Unnamed: 0', 'btag',  'brand'])
39/12:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
39/13:
log.sort_values(['user', 'time_stamp'], inplace=True)
def xf(d):
    print(d)
log.groupby('user').apply(xf)[:10]
39/14:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

df['d - a'] = df['d'][1:] - df['d'][:-1]

print(df)
39/15:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

df['d - a'] = df['d'][1:] - df['d'][:-1]

print(df['d'][1:] - df['d'][:-1])
39/16:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

df['d - a'] = df['d'][1:] - df['d'][:-1]

print(np.array(df['d'][1:]) - np.array(df['d'][:-1]))
39/17:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

df['d - a'] = np.array(df['d'][1:]) - np.array(df['d'][:-1])

print(np.array(df['d'][1:]) - np.array(df['d'][:-1]))
39/18:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

df['d - a'] = np.array(df['d'][1:]) - np.array(df['d'][:-1])

print(df)
39/19:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

df['d - a'] = np.array(df['d'][1:]) - np.array(df['d'][:-1])+np.array([0])

print(df)
39/20:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

df['d - a'] = np.array(df['d'][1:]) - np.array(df['d'])

print(df)
39/21:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])

np.array(df['d'][1:]) - np.array(df['d'])
39/22: df.index
39/23: df.index[-1]
39/24:
df = pd.DataFrame([[10,6,7,8],
                   [1,9,12,14],
                   [5,8,10,6]],
                  columns = ['a','b','c','d'])
diff = np.array(df['d'][1:]) - np.array(df['d'][:-1])
df = df.drop([df.index[-1]])
df['diff'] = diff
print(df)
39/25:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
39/26:
log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    sorted_log['interval'] = sorted_log.time_stamp
    diff = np.array(sorted_log['time_stamp'][1:]) - np.array(sorted_log['time_stamp'][:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['diff'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
39/27:
log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'][1:]) - np.array(sorted_log['time_stamp'][:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
39/28:

log.iloc[:1000].groupby('user').apply(gen_user_interval)
39/29:
log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'][1:]) - np.array(sorted_log['time_stamp'][:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
log.iloc[:10000].groupby('user').apply(gen_user_interval)
39/30: applyParallel(log.iloc[:10000].groupby('user'), gen_user_interval)
39/31: applyParallel(log.iloc[:100000].groupby('user'), gen_user_interval)
39/32:
log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].loc[1:]) - np.array(sorted_log['time_stamp'].loc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
log.iloc[:10000].groupby('user').apply(gen_user_interval)
39/33: applyParallel(log.iloc[:100000].groupby('user'), gen_user_interval)
39/34:
log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].loc[1:]) - np.array(sorted_log['time_stamp'].loc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
#log.iloc[:10000].groupby('user').apply(gen_user_interval)
39/35:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].loc[1:]) - np.array(sorted_log['time_stamp'].loc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
log.iloc[:10000].groupby('user').apply(gen_user_interval)
39/36: log[:1]
39/37:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
log.iloc[:10000].groupby('user').apply(gen_user_interval)
39/38:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
log.iloc[:100000].groupby('user').apply(gen_user_interval)
39/39: applyParallel(log.iloc[:100000].groupby('user'), gen_user_interval)
39/40:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)


# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
log.iloc[:10000000].groupby('user').apply(gen_user_interval)
39/41: applyParallel(log.iloc[:10000000].groupby('user'), gen_user_interval)
39/42: applyParallel(log.groupby('user'), gen_user_interval)
39/43:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    cate_interval['threshold'] = iforest_remove_outliner(interval_list)
    return cate_interval.threshold
    
# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
applyParallel(applyParallel(log[:1000000].groupby('user'), gen_user_interval).grouby('cate'), gen_cate_interval_threshold)
39/44:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    cate_interval['threshold'] = iforest_remove_outliner(interval_list)
    return cate_interval.threshold
    
# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
applyParallel(applyParallel(log[:1000000].groupby('user'), gen_user_interval).groupby('cate'), gen_cate_interval_threshold)
39/45:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    cate_interval['threshold'] = iforest_remove_outliner(interval_list)
    return print(cate_interval)
    
# applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
applyParallel(applyParallel(log[:1000000].groupby('user'), gen_user_interval).groupby('cate'), gen_cate_interval_threshold)
39/46:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    cate_interval['threshold'] = iforest_remove_outliner(interval_list)
    return print(cate_interval)
    
x = applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
39/47:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return intervals

x = applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
39/48:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
39/49:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    # intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    # return pd.DataFrame(data=intervals)
    print(threshold)

x = applyParallel(log.iloc[:1000].groupby('user'), gen_user_interval)
applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
39/50:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.iloc[:100000].groupby('user'), gen_user_interval)
applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
39/51:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in dfGrouped)
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.iloc[:1000000].groupby('user'), gen_user_interval)
applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
39/52:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
39/53:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
39/54:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl 
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
39/55:

cate_intervals.intervals
39/56:

cate_intervals.interval
39/57:

cate_intervals.interval.tolist()
39/58:

list(map(max, cate_intervals.interval.tolist()))
39/59:

list(map(max, cate_intervals.interval.tolist())).sort()
39/60:

sorted(list(map(max, cate_intervals.interval.tolist())))
39/61:

sorted(list(map(max, cate_intervals.interval.tolist())))[-1]
39/62:

sorted(list(map(max, cate_intervals.interval.tolist())))[-1]/60
39/63:

sorted(list(map(max, cate_intervals.interval.tolist())))[-1]/60/60
39/64:
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    if len(interval_list) < 10:
        threshold = iforest_remove_outliner(interval_list)
        intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
        return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
39/65:
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    if len(interval_list) < 2:
        threshold = iforest_remove_outliner(interval_list)
        intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
        return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
39/66:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl
39/67:
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    if len(interval_list) < 2:
        threshold = iforest_remove_outliner(interval_list)
        intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
        return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
39/68:
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    if len(interval_list) > 1:
        threshold = iforest_remove_outliner(interval_list)
        intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
        return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
39/69:
mean_interval = []
for i in cate_interval.keys():
    intervals = cate_interval[i]
    mean_interval.append(round(sum(intervals) / len(intervals),2))
mean_interval
39/70:

sorted(list(map(max, cate_intervals.interval.tolist())))
39/71: log
39/72: log.pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')
39/73: log.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')
39/74: pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')
39/75:
log = pd.read_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')
log
39/76: %store -r log
39/77: log
39/78: log.to_csv('/Users/yuxuanyang/Downloads/DSIN-master/code/logs.csv')
39/79: pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_' + str(FRAC) + '.pkl')
40/1: cate_intervals.interval.tolist()
40/2: cate_intervals
40/3:
cate_interval = {}
user_list = np.array(log.user.unique())
40/4:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
40/5: log
40/6: %store -r log
40/7: log
40/8: log = log.drop(columns = ['btag',  'brand'])
40/9: log[:1]
40/10:
cate_interval = {}
user_list = np.array(log.user.unique())
40/11:
cate_interval = {}
user_list = np.array(log.user.unique())
user_list
40/12:
cate_interval = {}
user_list = np.array(log.user.unique())
len(user_list)
40/13:
cate_interval = {}
user_list = np.array(log.cate.unique())
len(user_list)
40/14:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
40/15:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return tl
40/16:
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    # if len(interval_list) > 1:
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
40/17: %store cate_intervals
40/18:
cate_interval = {}
for i in cate_intervals.cate:
    cate_interval[i] = max(cate_intervals[cate_intervals.cate == i].interval)
cate_interval
40/19:
cate_interval = {}
for i in cate_intervals.cate:
    cate_interval[i] = max(cate_intervals.interval[cate_intervals.cate == i])
cate_interval
40/20:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return max(tl)
40/21:
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    # if len(interval_list) > 1:
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
40/22: pd.to_pickle(cate_intervals,'/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/cate_intervals' )
40/23: cate_intervals.set_index('cate')
40/24: pd.to_pickle(cate_intervals,'/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/cate_intervals' )
40/25: pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/cate_intervals')
40/26: cate_intervals
40/27: cate_intervals = cate_intervals.set_index('cate')
40/28: pd.to_pickle(cate_intervals,'/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/cate_intervals' )
40/29: pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/cate_intervals')
40/30: cate_intervals
40/31: cate_intervals.loc[1, interval]
40/32: cate_intervals.loc[1, 1]
40/33: cate_intervals.columns
40/34: cate_intervals.loc[1,0]
40/35: cate_intervals.iloc[1,0]
40/36: cate_intervals.iloc[0,0]
40/37: cate_intervals.at['1', 'interval']
40/38: cate_intervals.at[1, 'interval']
40/39: cate_intervals.index
41/1:
user_hist_session = {}
    #所有user_hist_session_0.25_dsin 文件数
FILE_NUM = len(
    list(filter(lambda x: x.startswith('user_hist_session_' + str(FRAC) + '_dsin_'),
                os.listdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'))))

for i in range(FILE_NUM):
    #读取每个hist session文件，并追加到后面
    user_hist_session_ = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_hist_session_' + str(FRAC) + '_dsin_' + str(i) + '.pkl')  # 19,34
    user_hist_session.update(user_hist_session_)
41/2:
user_hist_session = {}
FRAC = 0.25
    #所有user_hist_session_0.25_dsin 文件数
FILE_NUM = len(
    list(filter(lambda x: x.startswith('user_hist_session_' + str(FRAC) + '_dsin_'),
                os.listdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'))))

for i in range(FILE_NUM):
    #读取每个hist session文件，并追加到后面
    user_hist_session_ = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_hist_session_' + str(FRAC) + '_dsin_' + str(i) + '.pkl')  # 19,34
    user_hist_session.update(user_hist_session_)
41/3:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
41/4:
user_hist_session = {}
    #所有user_hist_session_0.25_dsin 文件数
FILE_NUM = len(
    list(filter(lambda x: x.startswith('user_hist_session_' + str(FRAC) + '_dsin_'),
                os.listdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'))))

for i in range(FILE_NUM):
    #读取每个hist session文件，并追加到后面
    user_hist_session_ = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_hist_session_' + str(FRAC) + '_dsin_' + str(i) + '.pkl')  # 19,34
    user_hist_session.update(user_hist_session_)
41/5: user_hist_session
41/6:
new_user_hist_session = {}
    #所有user_hist_session_0.25_dsin 文件数
FILE_NUM = len(
    list(filter(lambda x: x.startswith('new_user_hist_session_' + str(FRAC) + '_dsin_'),
                os.listdir('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/'))))

for i in range(FILE_NUM):
    #读取每个hist session文件，并追加到后面
    new_user_hist_session_ = pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/new_user_hist_session_' + str(FRAC) + '_dsin_' + str(i) + '.pkl')  # 19,34
    new_user_hist_session.update(user_hist_session_)
41/7: new_user_hist_session
41/8: len(new_user_hist_session)
41/9: len(user_hist_session)
41/10:
pd.read_pickle(
        '/Users/yuxuanyang/Downloads/DSIN-master/model_input/dsin_input_0.25_5.pkl')
42/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
42/2: %store -r log
42/3: log
42/4: pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_0.25.pkl')
43/1: print('hello\n\r')
43/2: print('hello\n0\r0')
43/3: print('hello\r0')
43/4: print('hello\r 0')
43/5: print('hello\r')
43/6: print('hello\n0')
43/7: print("\r\nHello world\r\n")
43/8: print("AAAA\r\nHello world\r\nAAAA")
43/9: print("AA\r\nHello world\r\nAAAA")
43/10: print("AA\nHello world\r\nAAAA")
43/11: print("AA\rHello world\nAAAA")
44/1:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(1, shorter):
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
        else:
            return common
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/2: main( ['apple', 'let'])
44/3: main( 2, ['apple', 'let'])
44/4: ifCombine('apple', 'let')
44/5:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(1, shorter):
        print(a[:i])
        print(b[-i:])
        print(b[:i])
        print(a[-i:])
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
        else:
            return common
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/6: main( 2, ['apple', 'let'])
44/7: ifCombine('apple', 'let')
44/8:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(1, shorter):
        print(i)
        print(a[:i])
        print(b[-i:])
        print(b[:i])
        print(a[-i:])
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
        else:
            return common
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/9: main( 2, ['apple', 'let'])
44/10:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(1, shorter,1):
        print(i)
        print(a[:i])
        print(b[-i:])
        print(b[:i])
        print(a[-i:])
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
        else:
            return common
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/11: main( 2, ['apple', 'let'])
44/12:
for i in range(1,7):
    print(i)
44/13:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):
        print(i)
        print(a[:i])
        print(b[-i:])
        print(b[:i])
        print(a[-i:])
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
        else:
            return common
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/14: main( 2, ['apple', 'let'])
44/15:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):
        print(i)
        print(a[:i])
        print(b[-i:])
        print(b[:i])
        print(a[-i:])
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
        else:
            return
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/16: main( 2, ['apple', 'let'])
44/17:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):
        print(i)
        print(a[:i])
        print(b[-i:])
        print(b[:i])
        print(a[-i:])
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/18: main( 2, ['apple', 'let'])
44/19:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):

        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/20: main( 2, ['apple', 'let'])
44/21: ifCombine('apple', 'let')
44/22: ifCombine('international', 'allofyou')
44/23: ifCombine('international', 'allofyou')
44/24:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):

        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/25: ifCombine('international', 'allofyou')
44/26: main( 2, ['apple', 'let'])
44/27: main( 2, ['international', 'allofyou', 'abb', 'ccc'])
44/28: main( 4, ['international', 'allofyou', 'abb', 'ccc'])
44/29:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):

        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/30:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i
        else:
            return common
    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/31: main( 4, ['international', 'allofyou', 'abb', 'ccc'])
44/32: ifCombine('international', 'allofyou')
44/33:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(shorter, 1, -1):
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i

    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/34: main( 4, ['international', 'allofyou', 'abb', 'ccc'])
44/35:
def ifCombine(a, b):
    shorter = min(len(a), len(b))
    common = 0
    for i in range(1, shorter):
        if a[:i] == b[-i:] and i > common:
            common = i     
        elif a[-i:] == b[:i] and i > common:
            common = i

    return common
def main(N, words):
    if N == 2:
        a, b = words
        common = ifCombine(a, b)
        if common > 0:
            print(len(a) + len(b) - common)
        if common == 0:
            print(-1)
    if N > 2:
        leng = 0
        for i in range(N):
            for j in range(i, N):
                a, b = words[i], words[j]
                common = ifCombine(a, b)
                if common > 0 and len(a) + len(b) - common > leng:
                    leng = len(a) + len(b) - common
        if leng == 0:
            print(-1)
        else:
            print(leng)
44/36: main( 4, ['international', 'allofyou', 'abb', 'ccc'])
44/37: ifCombine('international', 'allofyou')
46/1:
op = 1
for i in range(365, 315, -1):
    op = op * i
op
46/2:
op = 1
for i in range(365, 315, -1):
    op = op * i / 365
46/3:
op = 1
for i in range(365, 315, -1):
    op = op * i / 365
op
46/4:
op = 1
for i in range(365, 315, -1):
    op = op * i / 365
1-op
46/5:
op = 1
for i in range(365, 314, -1):
    op = op * i / 365
1-op
47/1:
import sys
def Counter(it):
    counts = {}
    for item in it:
        counts[item] = counts.get(item, 0) + 1
    return counts
def main(N, edges):
    square_edge_nums = 4
    if N < square_edge_nums:
        print(NO)
    else:
        edges_counter = Counter(edges)
        able_edges = dict(filter(lambda item: item[1] >= square_edge_nums, edges_counter.items()))
        if able_edges: print('YES')
        else: print('NO')
if __name__ == '__main__':
    N = int(input())
    edges = list(map(int, input().rstrip('\r\n').split(' ')))
    main(N, edges)
47/2:
import sys
# def Counter(it):
#     counts = {}
#     for item in it:
#         counts[item] = counts.get(item, 0) + 1
#     return counts
# def main(N, edges):
#     square_edge_nums = 4
#     if N < square_edge_nums:
#         print(NO)
#     else:
#         edges_counter = Counter(edges)
#         able_edges = dict(filter(lambda item: item[1] >= square_edge_nums, edges_counter.items()))
#         if able_edges: print('YES')
#         else: print('NO')
# if __name__ == '__main__':
#     N = int(input())
#     edges = list(map(int, input().rstrip('\r\n').split(' ')))
#     main(N, edges)
48/1:
op = 1
for p in range(365, 265, -1):
    op = p * op / 365
op
48/2:
op = 1
for p in range(365, 315, -1):
    op = p * op / 365
op
48/3:
op = 1
for p in range(365, 315, -1):
    op = p * op / 365
1-op
48/4:
op = 1
for p in range(365, 265, -1):
    op = p * op / 365
1-op
48/5:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
   # print(grid)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    paths = []
    vis = []
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1 and path not in paths:
            paths.append(path)
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))
    return paths
if __name__ == '__main__':
    R, C= list(map(int, input().split(' ')))
    lines = []
    for l in sys.stdin:
        lines.append(l.rstrip('\r\n'))
    print(main(R, C, lines))
49/1:
op = 1
for p in range(365, 265, -1):
    op = p * op / 365
1-op
49/2:
op = 1
for p in range(365, 265, -1):
    op = p * op / 365
1-op
49/3:
op = 1
for p in range(365, 265, -1):
    op = p * op / 365
1-op
49/4:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
   # print(grid)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    paths = []
    vis = []
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1 and path not in paths:
            paths.append(path)
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))
    return paths
if __name__ == '__main__':
    R, C= list(map(int, input().split(' ')))
    lines = []
    for l in sys.stdin:
        lines.append(l.rstrip('\r\n'))
    print(main(R, C, lines))
50/1:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
   # print(grid)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    paths = []
    vis = []
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1 and path not in paths:
            paths.append(path)
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))
    return paths
# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/2:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
main(R, C, lines)
50/3:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
   # print(grid)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1 and path not in paths:
            return path
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))

# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/4:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
main(R, C, lines)
50/5:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
print(main(R, C, lines))
50/6:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    print(grid)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1 and path not in paths:
            return path
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))

# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/7:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
print(main(R, C, lines))
50/8:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    print(grid)
    print(word)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1 and path not in paths:
            return path
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))

# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/9:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
print(main(R, C, lines))
50/10:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    print(grid)
    print(word)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1:
            return path
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))

# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/11:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
print(main(R, C, lines))
50/12:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    print(grid)
    print(word)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    path = []
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) -1:
            return path
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<= i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx]+1:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in vis:
                    queue.append(((i,j), next_path, idx + 1))

# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/13:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
print(main(R, C, lines))
50/14:
def main(r, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    print(grid)
    print(word)
    queue = []
    R, C = len(grid), len(grid[0])
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1:
            return path
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                next_path = path.copy()
                next_path.append((i,j))
                queue.append(((i,j), next_path, idx + 1))
        

# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/15:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
print(main(R, C, lines))
50/16:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    print(grid)
    print(word)
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1:
            return path
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                next_path = path.copy()
                next_path.append((i,j))
                queue.append(((i,j), next_path, idx + 1))
        

# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/17:
R = 3
C = 3
lines = ['abc', 'def', 'ghi', 'j']
print(main(R, C, lines))
50/18:
R = 6
C = 10
lines = [
"abccbaabcb",
"cbaababcbc",
"ababccbccb",
"abcabcbacc",
"babcbacbac",
"bbacbacbac",
"abc"]
print(main(R, C, lines))
50/19:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    # print(grid)
    # print(word)
    paths = []
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1:
            paths.append(path)
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                next_path = path.copy()
                next_path.append((i,j))
                queue.append(((i,j), next_path, idx + 1))
    return paths
# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/20:
R = 6
C = 10
lines = [
"abccbaabcb",
"cbaababcbc",
"ababccbccb",
"abcabcbacc",
"babcbacbac",
"bbacbacbac",
"abc"]
print(main(R, C, lines))
50/21:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    # print(grid)
    # print(word)
    paths = []
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1:
            paths.append(path)
        r, c = char
        for i, j in [(r, c+1),(r+1,c)]:
            if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                next_path = path.copy()
                next_path.append((i,j))
                if next_path not in paths:
                    queue.append(((i,j), next_path, idx + 1))
    return paths
# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/22:
R = 6
C = 10
lines = [
"abccbaabcb",
"cbaababcbc",
"ababccbccb",
"abcabcbacc",
"babcbacbac",
"bbacbacbac",
"abc"]
print(main(R, C, lines))
50/23:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    # print(grid)
    # print(word)
    paths = []
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1:
            paths.append(path)
        else:
            r, c = char
            for i, j in [(r, c+1),(r+1,c)]:
                if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                    next_path = path.copy()
                    next_path.append((i,j))
                    if next_path not in paths:
                        queue.append(((i,j), next_path, idx + 1))
    return paths
# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/24:
R = 6
C = 10
lines = [
"abccbaabcb",
"cbaababcbc",
"ababccbccb",
"abcabcbacc",
"babcbacbac",
"bbacbacbac",
"abc"]
print(main(R, C, lines))
50/25:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    # print(grid)
    # print(word)
    paths = []
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1:
            paths.append(path)
        else:
            r, c = char
            for i, j in [(r, c+1),(r+1,c)]:
                if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                    next_path = path.copy()
                    next_path.append((i,j))
                    if next_path not in paths:
                        queue.append(((i,j), next_path, idx + 1))
    return len(paths)
# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/26:
R = 6
C = 10
lines = [
"abccbaabcb",
"cbaababcbc",
"ababccbccb",
"abcabcbacc",
"babcbacbac",
"bbacbacbac",
"abc"]
print(main(R, C, lines))
50/27:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    # print(grid)
    # print(word)
    paths = []
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1 and  path not in paths:
            paths.append(path)
        else:
            r, c = char
            for i, j in [(r, c+1),(r+1,c)]:
                if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                    next_path = path.copy()
                    next_path.append((i,j))
                    if next_path not in paths:
                        queue.append(((i,j), next_path, idx + 1))
    return len(paths)
# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/28:
R = 6
C = 10
lines = [
"abccbaabcb",
"cbaababcbc",
"ababccbccb",
"abcabcbacc",
"babcbacbac",
"bbacbacbac",
"abc"]
print(main(R, C, lines))
50/29:
def main(R, C, lines):
    word = lines[-1]
    grid = list(map(lambda x: list(x), lines[:R]))
    # print(grid)
    # print(word)
    paths = []
    queue = []
    for r in range(R):
        for c in range(C):
            if grid[r][c] == word[0]:
                queue.append(((r,c), [(r,c)], 0))
    while queue:
        char, path, idx = queue.pop(0)
        if idx == len(word) - 1 and path not in paths:
            paths.append(path)
        else:
            r, c = char
            for i, j in [(r, c+1),(r+1,c)]:
                if 0<=i < R and 0<j<C and (i,j) not in path and grid[i][j] == word[idx+1]:
                    next_path = path.copy()
                    next_path.append((i,j))
                    if next_path not in paths:
                        queue.append(((i,j), next_path, idx + 1))
    return paths
# if __name__ == '__main__':
#     R, C= list(map(int, input().split(' ')))
#     lines = []
#     for l in sys.stdin:
#         lines.append(l.rstrip('\r\n'))
#     print(main(R, C, lines))
50/30:
R = 6
C = 10
lines = [
"abccbaabcb",
"cbaababcbc",
"ababccbccb",
"abcabcbacc",
"babcbacbac",
"bbacbacbac",
"abc"]
print(main(R, C, lines))
53/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
53/2: user_hist_session_ = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_hist_session_0.25_dsin_1.pkl')
53/3: user_hist_session_ = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/new_user_hist_session_0.25_dsin_1.pkl')
53/4:
user_hist_session_ = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_datauser_hist_session_0.25_dsin_1.pkl')
user_hist_session_
53/5:
user_hist_session_ = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/user_hist_session_0.25_dsin_1.pkl')
user_hist_session_
53/6:
user_hist_session_ = pd.read_pickle('/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/new_user_hist_session_0.25_dsin_1.pkl')
user_hist_session_
55/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
55/2: %store -r log
55/3: print(len(log))
55/4: log1 = pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_0.25.pkl')
55/5: len(log)
55/6: len(log1)
55/7: len(log)
55/8: log.index
55/9:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
55/10: log1 = pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_0.25.pkl')
55/11: len(log1)
56/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
56/2: log1 = pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_0.25.pkl')
56/3: log1 = pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_0.25.pkl')
57/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
57/2: log = pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_0.25.pkl')
58/1:
import os
import sqlite3
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from joblib import Parallel, delayed
import multiprocessing
FRAC = 0.25
58/2: log = pd.to_pickle(log, '/Users/yuxuanyang/Downloads/DSIN-master/sampled_data/behavior_log_pv_user_filter_enc_0.25.pkl')
58/3: %store -r log
58/4: log
58/5:
cate_interval = {}
user_list = np.array(log.cate.unique())
len(user_list)
58/6:
cate_interval = {}
def gen_user_interval_list(user_id, log, cate_interval):
    user_log = log[log.user == user_id]
    sorted_log = user_log.sort_values(by =['time_stamp'])
    time_array = np.array(sorted_log.time_stamp)
    time_diff = time_array[1:] - time_array[:-1]
    cate_list = sorted_log.cate.tolist()
   # return cate_list, time_diff
    for i in range(len(time_diff)):
        if cate_list[i] not in cate_interval.keys():
            cate_interval[cate_list[i]] = []
        cate_interval[cate_list[i]].append(time_diff[i])


for u in tqdm(user_list[:100]):
    gen_user_interval_list(u,log,cate_interval)
58/7:
##log.sort_values(['user', 'time_stamp'], inplace=True)
def gen_user_interval(user_log):
    sorted_log = user_log.sort_values(by =['time_stamp'])
    diff = np.array(sorted_log['time_stamp'].iloc[1:]) - np.array(sorted_log['time_stamp'].iloc[:-1])
    sorted_log = sorted_log.drop([sorted_log.index[-1]])
    sorted_log['interval'] = diff
    return sorted_log.drop(columns = ['user'])

def applyParallel(dfGrouped, func):
    retLst = Parallel(n_jobs=multiprocessing.cpu_count())(delayed(func)(group) for name, group in tqdm(dfGrouped))
    return pd.concat(retLst)
#删除异常值 用异常分数的0.5作为阀值
def iforest_remove_outliner(intervel_list):
    sll = intervel_list
    X = np.array(sll).reshape(len(sll),1)
    clf = IsolationForest(random_state=0).fit(X)
    predict_result=clf.predict(X)
    tl=[]
    for i in range(len(sll)):
        if predict_result[i]==1:
            tl.append(sll[i])
    return max(tl)
58/8:
def gen_cate_interval_threshold(cate_interval):
    interval_list = cate_interval.interval.tolist()
    # if len(interval_list) > 1:
    threshold = iforest_remove_outliner(interval_list)
    intervals = {'cate': cate_interval.cate.iloc[0], 'interval': [threshold]}
    return pd.DataFrame(data=intervals)

x = applyParallel(log.groupby('user'), gen_user_interval)
cate_intervals = applyParallel(x.groupby('cate'), gen_cate_interval_threshold)
cate_intervals
59/1: % history
59/2: %history
59/3: %history -g
   1: %history -g
   2: %history -g -f filename
